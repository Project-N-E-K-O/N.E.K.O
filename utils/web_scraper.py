"""
ç½‘ç»œçˆ¬è™«æ¨¡å—ï¼Œç”¨äºè·å–å„å¹³å°çš„çƒ­é—¨å†…å®¹
æ”¯æŒåŸºäºåŒºåŸŸçš„å†…å®¹è·å–ï¼š
- ä¸­æ–‡åŒºåŸŸï¼šBç«™å’Œå¾®åš
- éä¸­æ–‡åŒºåŸŸï¼šRedditå’ŒTwitter
åŒæ—¶æ”¯æŒè·å–æ´»è·ƒçª—å£æ ‡é¢˜å’Œæœç´¢åŠŸèƒ½
"""
import asyncio
import httpx
import random
import re
import platform
from typing import Dict, List, Any, Optional, Union
import logging
from urllib.parse import quote
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage
from bs4 import BeautifulSoup
import os
from pathlib import Path
import json

# ä» language_utils å¯¼å…¥åŒºåŸŸæ£€æµ‹åŠŸèƒ½
try:
    from utils.language_utils import is_china_region
except ImportError:
    # å¦‚æœ language_utils ä¸å¯ç”¨ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ
    import locale
    def is_china_region() -> bool:
        """
        åŒºåŸŸæ£€æµ‹å›é€€æ–¹æ¡ˆ

        ä»…å¯¹ä¸­å›½å¤§é™†åœ°åŒºè¿”å›Trueï¼ˆzh_cnåŠå…¶å˜ä½“ï¼‰
        æ¸¯æ¾³å°åœ°åŒºï¼ˆzh_tw, zh_hkï¼‰è¿”å›False
        Windows ä¸­æ–‡ç³»ç»Ÿè¿”å› True
        """
        mainland_china_locales = {'zh_cn', 'chinese_china', 'chinese_simplified_china'}

        def normalize_locale(loc: str) -> str:
            """æ ‡å‡†åŒ–localeå­—ç¬¦ä¸²ï¼šå°å†™ã€æ›¿æ¢è¿å­—ç¬¦ã€å»é™¤ç¼–ç """
            if not loc:
                return ''
            loc = loc.lower()
            loc = loc.replace('-', '_')
            if '.' in loc:
                loc = loc.split('.')[0]
            return loc

        def check_locale(loc: str) -> bool:
            """æ£€æŸ¥æ ‡å‡†åŒ–åçš„localeæ˜¯å¦ä¸ºä¸­å›½å¤§é™†"""
            normalized = normalize_locale(loc)
            if not normalized:
                return False
            if normalized in mainland_china_locales:
                return True
            if normalized.startswith('zh_cn'):
                return True
            if 'chinese' in normalized and 'china' in normalized:
                return True
            return False

        try:
            try:
                system_locale = locale.getlocale()[0]
                if system_locale and check_locale(system_locale):
                    return True
            except Exception:
                pass

            try:
                default_locale = locale.getdefaultlocale()[0]
                if default_locale and check_locale(default_locale):
                    return True
            except Exception:
                pass

            return False
        except Exception:
            return False

logger = logging.getLogger(__name__)

# User-Agentæ± ï¼Œéšæœºé€‰æ‹©ä»¥é¿å…è¢«è¯†åˆ«
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
]

def get_random_user_agent() -> str:
    """éšæœºè·å–ä¸€ä¸ªUser-Agent"""
    return random.choice(USER_AGENTS)


def _get_bilibili_credential():
    """
    ä»æ–‡ä»¶åŠ è½½Bilibiliè®¤è¯ä¿¡æ¯ï¼Œè¿”å›Credentialå¯¹è±¡
    
    æ”¯æŒä»ä»¥ä¸‹ä½ç½®è¯»å–cookiesï¼š
    1. ~/bilibili_cookies.json
    2. config/bilibili_cookies.json
    3. ./bilibili_cookies.json
    
    Returns:
        Credentialå¯¹è±¡ï¼Œå¦‚æœåŠ è½½å¤±è´¥åˆ™è¿”å›None
    """
    try:
        from bilibili_api import Credential
        
        # æŸ¥æ‰¾å¯èƒ½çš„cookieæ–‡ä»¶ä½ç½®
        possible_paths = [
            Path(os.path.expanduser('~')) / 'bilibili_cookies.json',
            Path('config') / 'bilibili_cookies.json',
            Path('.') / 'bilibili_cookies.json',
        ]
        
        for cookie_file in possible_paths:
            if cookie_file.exists():
                with open(cookie_file, 'r', encoding='utf-8') as f:
                    cookie_data = json.load(f)
                    
                    # æå–å¿…è¦çš„è®¤è¯ä¿¡æ¯
                    cookies = {}
                    
                    # EditThisCookie/Cookie-Editoræ ¼å¼ (æ•°ç»„)
                    if isinstance(cookie_data, list):
                        for cookie in cookie_data:
                            if cookie.get('domain', '').endswith('bilibili.com'):
                                cookies[cookie['name']] = cookie['value']
                    
                    # ç®€å•çš„é”®å€¼å¯¹æ ¼å¼
                    elif isinstance(cookie_data, dict):
                        cookies = cookie_data
                    
                    # åˆ›å»ºCredentialå¯¹è±¡
                    if cookies:
                        sessdata = cookies.get('SESSDATA', '')
                        bili_jct = cookies.get('bili_jct', '')
                        buvid3 = cookies.get('buvid3', '')
                        dedeuserid = cookies.get('DedeUserID', '')
                        
                        if sessdata:
                            credential = Credential(
                                sessdata=sessdata,
                                bili_jct=bili_jct,
                                buvid3=buvid3,
                                dedeuserid=dedeuserid
                            )
                            logger.info(f"âœ… æˆåŠŸä»æ–‡ä»¶åŠ è½½ Bilibili è®¤è¯ä¿¡æ¯: {cookie_file}")
                            return credential
                        else:
                            logger.warning(f"âš ï¸ Cookieæ–‡ä»¶ç¼ºå°‘SESSDATA: {cookie_file}")
    except ImportError:
        logger.debug("bilibili_api åº“æœªå®‰è£…")
    except Exception as e:
        logger.debug(f"ä»æ–‡ä»¶åŠ è½½è®¤è¯ä¿¡æ¯å¤±è´¥: {e}")
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°cookieæ–‡ä»¶ï¼Œè®°å½•æç¤ºä¿¡æ¯
    logger.info("ğŸ’¡ æç¤ºï¼šè¦ä½¿ç”¨ä¸ªæ€§åŒ–Bç«™æ¨èï¼Œè¯·å¯¼å‡ºcookiesåˆ°ä»¥ä¸‹ä»»ä¸€ä½ç½®ï¼š")
    logger.info(f"   1. {Path(os.path.expanduser('~')) / 'bilibili_cookies.json'}")
    logger.info(f"   2. {Path('config') / 'bilibili_cookies.json'}")
    logger.info("   ä½¿ç”¨æµè§ˆå™¨æ‰©å±• 'EditThisCookie' æˆ– 'Cookie-Editor' å¯¼å‡ºä¸ºJSONæ ¼å¼")
    
    return None


async def fetch_bilibili_trending(limit: int = 30) -> Dict[str, Any]:
    """
    è·å–Bç«™é¦–é¡µæ¨èè§†é¢‘
    ä½¿ç”¨bilibili-apiåº“è·å–ä¸»é¡µè§†é¢‘æ¨è
    æ”¯æŒä¸ªæ€§åŒ–æ¨èï¼ˆå¦‚æœæä¾›äº†è®¤è¯ä¿¡æ¯ï¼‰
    """
    try:
        from bilibili_api import homepage
        
        # è·å–è®¤è¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        credential = _get_bilibili_credential()
        
        # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        await asyncio.sleep(random.uniform(0.1, 0.5))
        
        # ä½¿ç”¨bilibili-apiè·å–é¦–é¡µæ¨è
        # å¦‚æœæœ‰credentialï¼Œä¼šè·å–ä¸ªæ€§åŒ–æ¨èï¼›å¦åˆ™è·å–é€šç”¨æ¨è
        result = await homepage.get_videos(credential=credential)
        
        videos = []
        if result and 'item' in result:
            items = result['item']
            for item in items[:limit]:
                # æå–è§†é¢‘ä¿¡æ¯
                bvid = item.get('bvid', '')
                # æœ‰äº›é¡¹ç›®å¯èƒ½æ˜¯å¹¿å‘Šæˆ–å…¶ä»–ç±»å‹ï¼Œè·³è¿‡æ²¡æœ‰bvidçš„
                if not bvid:
                    continue
                    
                videos.append({
                    'title': item.get('title', ''),
                    'desc': item.get('desc', ''),
                    'author': item.get('owner', {}).get('name', ''),
                    'view': item.get('stat', {}).get('view', 0),
                    'like': item.get('stat', {}).get('like', 0),
                    'bvid': bvid,
                    'url': f'https://www.bilibili.com/video/{bvid}'
                })
                
                # å¦‚æœå·²ç»è·å–åˆ°è¶³å¤Ÿçš„è§†é¢‘ï¼Œåœæ­¢
                if len(videos) >= limit:
                    break
        
        if credential:
            logger.info(f"âœ… ä½¿ç”¨ä¸ªæ€§åŒ–æ¨èè·å–åˆ° {len(videos)} ä¸ªBç«™è§†é¢‘")
        else:
            logger.info(f"âœ… ä½¿ç”¨é»˜è®¤æ¨èè·å–åˆ° {len(videos)} ä¸ªBç«™è§†é¢‘")
        
        return {
            'success': True,
            'videos': videos
        }
        
    except ImportError:
        logger.error("bilibili_api åº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install bilibili-api-python")
        return {
            'success': False,
            'error': 'bilibili_api åº“æœªå®‰è£…'
        }
    except Exception as e:
        logger.error(f"è·å–Bç«™æ¨èå¤±è´¥: {e}")
        import traceback
        logger.debug(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return {
            'success': False,
            'error': str(e)
        }




async def fetch_reddit_popular(limit: int = 10) -> Dict[str, Any]:
    """
    è·å–Redditçƒ­é—¨å¸–å­
    ä½¿ç”¨Redditçš„JSON APIè·å–r/popularçš„çƒ­é—¨å¸–å­
    
    Args:
        limit: è¿”å›å¸–å­çš„æœ€å¤§æ•°é‡
    
    Returns:
        åŒ…å«æˆåŠŸçŠ¶æ€å’Œå¸–å­åˆ—è¡¨çš„å­—å…¸
    """
    try:
        # Redditçš„JSON APIç«¯ç‚¹
        url = f"https://www.reddit.com/r/popular/hot.json?limit={limit}"
        
        headers = {
            'User-Agent': get_random_user_agent(),
            'Accept': 'application/json',
        }
        
        await asyncio.sleep(random.uniform(0.1, 0.5))
        
        async with httpx.AsyncClient(timeout=5.0, follow_redirects=True) as client:
            response = await client.get(url, headers=headers)
            response.raise_for_status()
            data = response.json()
            
            posts = []
            children = data.get('data', {}).get('children', [])
            
            for item in children[:limit]:
                post_data = item.get('data', {})
                
                # è·³è¿‡NSFWå†…å®¹
                if post_data.get('over_18'):
                    continue
                
                subreddit = post_data.get('subreddit', '')
                title = post_data.get('title', '')
                score = post_data.get('score', 0)
                num_comments = post_data.get('num_comments', 0)
                permalink = post_data.get('permalink', '')
                
                posts.append({
                    'title': title,
                    'subreddit': f"r/{subreddit}",
                    'score': _format_score(score),
                    'comments': _format_score(num_comments),
                    'url': f"https://www.reddit.com{permalink}" if permalink else ''
                })
            
            if posts:
                logger.info(f"ä»Redditè·å–åˆ°{len(posts)}æ¡çƒ­é—¨å¸–å­")
                return {
                    'success': True,
                    'posts': posts
                }
            else:
                return {
                    'success': False,
                    'error': 'Redditè¿”å›ç©ºæ•°æ®',
                    'posts': []
                }
                
    except httpx.TimeoutException:
        logger.exception("è·å–Redditçƒ­é—¨è¶…æ—¶")
        return {
            'success': False,
            'error': 'è¯·æ±‚è¶…æ—¶',
            'posts': []
        }
    except Exception as e:
        logger.exception(f"è·å–Redditçƒ­é—¨å¤±è´¥: {e}")
        return {
            'success': False,
            'error': str(e),
            'posts': []
        }


def _format_score(count: int) -> str:
    """æ ¼å¼åŒ–Redditåˆ†æ•°/è¯„è®ºæ•°"""
    if count >= 1_000_000:
        return f"{count / 1_000_000:.1f}M"
    elif count >= 1_000:
        return f"{count / 1_000:.1f}K"
    elif count > 0:
        return str(count)
    return "0"


async def fetch_weibo_trending(limit: int = 10) -> Dict[str, Any]:
    """
    è·å–å¾®åšçƒ­è®®è¯é¢˜
    ä¼˜å…ˆä½¿ç”¨s.weibo.comçƒ­æœæ¦œé¡µé¢ï¼ˆåˆ·æ–°é¢‘ç‡æ›´é«˜ï¼‰ï¼Œéœ€è¦Cookie
    å¦‚æœå¤±è´¥åˆ™å›é€€åˆ°å…¬å¼€API
    """
    from bs4 import BeautifulSoup
    
    # å¾®åšCookieé…ç½® - ç”¨äºè®¿é—®çƒ­æœé¡µé¢
    WEIBO_COOKIE = "SUB=_2AkMWJrkXf8NxqwJRmP8SxWjnaY12zwnEieKgekjMJRMxHRl-yj9jqmtbtRB6PaaX-IGp-AjmO6k5cS-OH2X9CayaTzVD"
    
    try:
        # ä¼˜å…ˆä½¿ç”¨s.weibo.comçƒ­æœé¡µé¢ï¼ˆåˆ·æ–°é¢‘ç‡æ›´é«˜ï¼‰
        url = "https://s.weibo.com/top/summary?cate=realtimehot"
        
        headers = {
            'User-Agent': get_random_user_agent(),
            'Referer': 'https://s.weibo.com/',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Cookie': WEIBO_COOKIE,
        }
        
        # æ·»åŠ éšæœºå»¶è¿Ÿ
        await asyncio.sleep(random.uniform(0.1, 0.5))
        
        async with httpx.AsyncClient(timeout=5.0, follow_redirects=True) as client:
            response = await client.get(url, headers=headers)
            response.raise_for_status()
            
            # æ£€æŸ¥æ˜¯å¦é‡å®šå‘åˆ°ç™»å½•é¡µé¢
            if 'passport' in str(response.url):
                logger.warning("å¾®åšCookieå¯èƒ½å·²è¿‡æœŸï¼Œå›é€€åˆ°å…¬å¼€API")
                return await _fetch_weibo_trending_fallback(limit)
            
            html = response.text
            soup = BeautifulSoup(html, 'html.parser')
            
            # è§£æçƒ­æœåˆ—è¡¨ (td-02 class)
            td_items = soup.find_all('td', class_='td-02')
            
            if not td_items:
                logger.warning("æœªæ‰¾åˆ°çƒ­æœæ•°æ®ï¼Œå›é€€åˆ°å…¬å¼€API")
                return await _fetch_weibo_trending_fallback(limit)
            
            trending_list = []
            for i, td in enumerate(td_items):
                if len(trending_list) >= limit:
                    break
                    
                a_tag = td.find('a')
                span = td.find('span')
                
                if a_tag:
                    word = a_tag.get_text(strip=True)
                    if not word:
                        continue
                    
                    # è·å–é“¾æ¥
                    href = a_tag.get('href', '')
                    # æ„å»ºå®Œæ•´URLï¼ˆç›¸å¯¹é“¾æ¥éœ€è¦åŠ ä¸ŠåŸŸåï¼‰
                    if href and not href.startswith('http'):
                        href = f"https://s.weibo.com{href}"
                    
                    # è§£æçƒ­åº¦å€¼
                    hot_text = span.get_text(strip=True) if span else ''
                    # çƒ­åº¦å¯èƒ½åŒ…å«ç±»å‹æ ‡ç­¾å¦‚"å‰§é›† 336075"ï¼Œéœ€è¦æå–æ•°å­—
                    import re
                    hot_match = re.search(r'(\d+)', hot_text)
                    raw_hot = int(hot_match.group(1)) if hot_match else 0
                    
                    # æå–æ ‡ç­¾ï¼ˆå¦‚"å‰§é›†"ã€"æ™šä¼š"ç­‰ï¼‰
                    note = re.sub(r'\d+', '', hot_text).strip() if hot_text else ''
                    
                    trending_list.append({
                        'word': word,
                        'raw_hot': raw_hot,
                        'note': note,
                        'rank': i + 1,
                        'url': href
                    })
            
            if trending_list:
                logger.info(f"æˆåŠŸä»s.weibo.comè·å–{len(trending_list)}æ¡çƒ­æœ")
                return {
                    'success': True,
                    'trending': trending_list
                }
            else:
                return await _fetch_weibo_trending_fallback(limit)
                
    except Exception as e:
        logger.warning(f"s.weibo.comçƒ­æœè·å–å¤±è´¥: {e}ï¼Œå›é€€åˆ°å…¬å¼€API")
        return await _fetch_weibo_trending_fallback(limit)


async def _fetch_weibo_trending_fallback(limit: int = 10) -> Dict[str, Any]:
    """
    å¾®åšçƒ­æœå›é€€æ–¹æ¡ˆ - ä½¿ç”¨å…¬å¼€çš„ajax API
    """
    try:
        url = "https://weibo.com/ajax/side/hotSearch"
        
        headers = {
            'User-Agent': get_random_user_agent(),
            'Referer': 'https://weibo.com',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'DNT': '1',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
        }
        
        await asyncio.sleep(random.uniform(0.1, 0.5))
        
        async with httpx.AsyncClient(timeout=5.0, follow_redirects=True) as client:
            response = await client.get(url, headers=headers)
            response.raise_for_status()
            data = response.json()
            
            if data.get('ok') == 1:
                trending_list = []
                realtime_list = data.get('data', {}).get('realtime', [])
                
                for item in realtime_list[:limit]:
                    if item.get('is_ad'):
                        continue
                    
                    word = item.get('word', '')
                    # æ„å»ºæœç´¢URL
                    search_url = f"https://s.weibo.com/weibo?q={quote(word)}" if word else ''
                    
                    trending_list.append({
                        'word': word,
                        'raw_hot': item.get('raw_hot', 0),
                        'note': item.get('note', ''),
                        'rank': item.get('rank', 0),
                        'url': search_url
                    })
                
                return {
                    'success': True,
                    'trending': trending_list[:limit]
                }
            else:
                logger.error("å¾®åšå…¬å¼€APIè¿”å›é”™è¯¯")
                return {
                    'success': False,
                    'error': 'å¾®åšAPIè¿”å›é”™è¯¯'
                }
                
    except httpx.TimeoutException:
        logger.exception("è·å–å¾®åšçƒ­è®®è¯é¢˜è¶…æ—¶")
        return {
            'success': False,
            'error': 'è¯·æ±‚è¶…æ—¶'
        }
    except Exception as e:
        logger.exception(f"è·å–å¾®åšçƒ­è®®è¯é¢˜å¤±è´¥: {e}")
        return {
            'success': False,
            'error': str(e)
        }


async def fetch_twitter_trending(limit: int = 10) -> Dict[str, Any]:
    """
    è·å–Twitter/Xçƒ­é—¨è¯é¢˜
    ä½¿ç”¨Twitterçš„æ¢ç´¢é¡µé¢è·å–çƒ­é—¨è¯é¢˜
    
    Args:
        limit: è¿”å›çƒ­é—¨è¯é¢˜çš„æœ€å¤§æ•°é‡
    
    Returns:
        åŒ…å«æˆåŠŸçŠ¶æ€å’Œçƒ­é—¨åˆ—è¡¨çš„å­—å…¸
    """
    try:
        # Twitteræ¢ç´¢/çƒ­é—¨é¡µé¢
        url = "https://twitter.com/explore/tabs/trending"
        
        headers = {
            'User-Agent': get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'DNT': '1',
        }
        
        await asyncio.sleep(random.uniform(0.1, 0.5))
        
        async with httpx.AsyncClient(timeout=5.0, follow_redirects=True) as client:
            response = await client.get(url, headers=headers)
            response.raise_for_status()
            html_content = response.text
            
            # ä»é¡µé¢è§£æçƒ­é—¨è¯é¢˜
            trending_list = []
            
            # å°è¯•ä»é¡µé¢çš„JSONæ•°æ®ä¸­æå–çƒ­é—¨è¯é¢˜
            trend_pattern = r'"trend":\{[^}]*"name":"([^"]+)"'
            tweet_count_pattern = r'"tweetCount":"([^"]+)"'
            
            trends = re.findall(trend_pattern, html_content)
            tweet_counts = re.findall(tweet_count_pattern, html_content)
            
            for i, trend in enumerate(trends[:limit]):
                if trend and not trend.startswith('#'):
                    trend = '#' + trend if not trend.startswith('@') else trend
                
                # æ„å»ºæœç´¢URL
                search_url = f"https://twitter.com/search?q={quote(trend)}" if trend else ''
                
                trending_list.append({
                    'word': trend,
                    'tweet_count': tweet_counts[i] if i < len(tweet_counts) else 'N/A',
                    'note': '',
                    'rank': i + 1,
                    'url': search_url
                })
            
            if trending_list:
                return {
                    'success': True,
                    'trending': trending_list
                }
            else:
                return await _fetch_twitter_trending_fallback(limit)
                
    except httpx.TimeoutException:
        logger.exception("è·å–Twitterçƒ­é—¨è¶…æ—¶")
        return {
            'success': False,
            'error': 'è¯·æ±‚è¶…æ—¶'
        }
    except Exception as e:
        logger.exception(f"è·å–Twitterçƒ­é—¨å¤±è´¥: {e}")
        return await _fetch_twitter_trending_fallback(limit)


async def _fetch_twitter_trending_fallback(limit: int = 10) -> Dict[str, Any]:
    """
    Twitterçƒ­é—¨çš„å›é€€æ–¹æ¡ˆ
    ä½¿ç”¨ç¬¬ä¸‰æ–¹æœåŠ¡è·å–çƒ­é—¨è¯é¢˜ï¼Œå› ä¸ºTwitterå®˜æ–¹APIéœ€è¦OAuthè®¤è¯
    """
    
    def _parse_trends24(soup: BeautifulSoup, limit: int) -> List[Dict[str, Any]]:
        """è§£æTrends24é¡µé¢"""
        trending_list = []
        trend_cards = soup.select('.trend-card__list li a')
        for i, item in enumerate(trend_cards[:limit]):
            trend_text = item.get_text(strip=True)
            if trend_text:
                search_url = f"https://twitter.com/search?q={quote(trend_text)}"
                trending_list.append({
                    'word': trend_text,
                    'tweet_count': 'N/A',
                    'note': '',
                    'rank': i + 1,
                    'url': search_url
                })
        return trending_list
    
    def _parse_getdaytrends(soup: BeautifulSoup, limit: int) -> List[Dict[str, Any]]:
        """è§£æGetDayTrendsé¡µé¢"""
        trending_list = []
        trend_items = soup.select('table.table tr td a')
        for i, item in enumerate(trend_items[:limit]):
            trend_text = item.get_text(strip=True)
            if trend_text:
                search_url = f"https://twitter.com/search?q={quote(trend_text)}"
                trending_list.append({
                    'word': trend_text,
                    'tweet_count': 'N/A',
                    'note': '',
                    'rank': i + 1,
                    'url': search_url
                })
        return trending_list
    
    # ç¬¬ä¸‰æ–¹çƒ­é—¨è¯é¢˜æºåˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
    fallback_sources = [
        {
            'name': 'Trends24',
            'url': 'https://trends24.in/',
            'parser': _parse_trends24
        },
        {
            'name': 'GetDayTrends',
            'url': 'https://getdaytrends.com/',
            'parser': _parse_getdaytrends
        }
    ]
    
    headers = {
        'User-Agent': get_random_user_agent(),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9',
    }
    
    # æŒ‰ä¼˜å…ˆçº§éå†æ‰€æœ‰æ•°æ®æº
    for source in fallback_sources:
        try:
            await asyncio.sleep(random.uniform(0.1, 0.3))
            
            async with httpx.AsyncClient(timeout=5.0, follow_redirects=True) as client:
                response = await client.get(source['url'], headers=headers)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    trending_list = source['parser'](soup, limit)
                    
                    if trending_list:
                        logger.info(f"ä»{source['name']}è·å–åˆ°{len(trending_list)}æ¡Twitterçƒ­é—¨")
                        return {
                            'success': True,
                            'trending': trending_list,
                            'source': source['name'].lower().replace(' ', '')
                        }
        except Exception as e:
            logger.warning(f"{source['name']}è·å–å¤±è´¥: {e}")
            continue
    
    # æ‰€æœ‰ç¬¬ä¸‰æ–¹æºéƒ½å¤±è´¥ï¼Œè¿”å›æç¤ºä¿¡æ¯
    logger.warning("æ‰€æœ‰Twitterçƒ­é—¨æ•°æ®æºå‡ä¸å¯ç”¨")
    return {
        'success': False,
        'error': 'Twitterçƒ­é—¨æ•°æ®æš‚æ—¶æ— æ³•è·å–ï¼Œè¯·ç¨åé‡è¯•æˆ–è®¿é—® twitter.com/explore',
        'trending': []
    }


async def fetch_trending_content(bilibili_limit: int = 10, weibo_limit: int = 10, 
                                  reddit_limit: int = 10, twitter_limit: int = 10) -> Dict[str, Any]:
    """
    æ ¹æ®ç”¨æˆ·åŒºåŸŸè·å–çƒ­é—¨å†…å®¹
    
    ä¸­æ–‡åŒºåŸŸï¼šè·å–Bç«™è§†é¢‘å’Œå¾®åšçƒ­è®®è¯é¢˜
    éä¸­æ–‡åŒºåŸŸï¼šè·å–Redditçƒ­é—¨å¸–å­å’ŒTwitterçƒ­é—¨è¯é¢˜
    
    Args:
        bilibili_limit: Bç«™è§†é¢‘æœ€å¤§æ•°é‡ï¼ˆä¸­æ–‡åŒºåŸŸï¼‰
        weibo_limit: å¾®åšè¯é¢˜æœ€å¤§æ•°é‡ï¼ˆä¸­æ–‡åŒºåŸŸï¼‰
        reddit_limit: Redditå¸–å­æœ€å¤§æ•°é‡ï¼ˆéä¸­æ–‡åŒºåŸŸï¼‰
        twitter_limit: Twitterè¯é¢˜æœ€å¤§æ•°é‡ï¼ˆéä¸­æ–‡åŒºåŸŸï¼‰
    
    Returns:
        åŒ…å«æˆåŠŸçŠ¶æ€å’Œçƒ­é—¨å†…å®¹çš„å­—å…¸
        ä¸­æ–‡åŒºåŸŸï¼š'bilibili' å’Œ 'weibo' é”®
        éä¸­æ–‡åŒºåŸŸï¼š'reddit' å’Œ 'twitter' é”®
    """
    try:
        # æ£€æµ‹ç”¨æˆ·åŒºåŸŸ
        china_region = is_china_region()
        
        if china_region:
            # Chinese region: Use Bilibili and Weibo
            logger.info("æ£€æµ‹åˆ°ä¸­æ–‡åŒºåŸŸï¼Œè·å–Bç«™å’Œå¾®åšçƒ­é—¨å†…å®¹")
            
            bilibili_task = fetch_bilibili_trending(bilibili_limit)
            weibo_task = fetch_weibo_trending(weibo_limit)
            
            
            bilibili_result, weibo_result = await asyncio.gather(
                bilibili_task, 
                weibo_task,
                return_exceptions=True
            )

            # å¤„ç†å¼‚å¸¸
            if isinstance(bilibili_result, Exception):
                logger.error(f"Bç«™çˆ¬å–å¼‚å¸¸: {bilibili_result}")
                bilibili_result = {'success': False, 'error': str(bilibili_result)}
            
            if isinstance(weibo_result, Exception):
                logger.error(f"å¾®åšçˆ¬å–å¼‚å¸¸: {weibo_result}")
                weibo_result = {'success': False, 'error': str(weibo_result)}
            
            # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªæˆåŠŸ
            if not bilibili_result.get('success') and not weibo_result.get('success'):
                return {
                    'success': False,
                    'error': 'æ— æ³•è·å–ä»»ä½•çƒ­é—¨å†…å®¹',
                    'region': 'china',
                    'bilibili': bilibili_result,
                    'weibo': weibo_result
                }
            
            return {
                'success': True,
                'region': 'china',
                'bilibili': bilibili_result,
                'weibo': weibo_result
            }
        else:
            # éä¸­æ–‡åŒºåŸŸï¼šä½¿ç”¨Redditå’ŒTwitter
            logger.info("æ£€æµ‹åˆ°éä¸­æ–‡åŒºåŸŸï¼Œè·å–Redditå’ŒTwitterçƒ­é—¨å†…å®¹")
            
            reddit_task = fetch_reddit_popular(reddit_limit)
            twitter_task = fetch_twitter_trending(twitter_limit)
            
            reddit_result, twitter_result = await asyncio.gather(
                reddit_task,
                twitter_task,
                return_exceptions=True
            )
            
            # å¤„ç†å¼‚å¸¸
            if isinstance(reddit_result, Exception):
                logger.error(f"Redditçˆ¬å–å¼‚å¸¸: {reddit_result}")
                reddit_result = {'success': False, 'error': str(reddit_result)}
            
            if isinstance(twitter_result, Exception):
                logger.error(f"Twitterçˆ¬å–å¼‚å¸¸: {twitter_result}")
                twitter_result = {'success': False, 'error': str(twitter_result)}
            
            # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªæˆåŠŸ
            if not reddit_result.get('success') and not twitter_result.get('success'):
                return {
                    'success': False,
                    'error': 'æ— æ³•è·å–ä»»ä½•çƒ­é—¨å†…å®¹',
                    'region': 'non-china',
                    'reddit': reddit_result,
                    'twitter': twitter_result
                }
            
            return {
                'success': True,
                'region': 'non-china',
                'reddit': reddit_result,
                'twitter': twitter_result
            }
        
    except Exception as e:
        logger.error(f"è·å–çƒ­é—¨å†…å®¹å¤±è´¥: {e}")
        return {
            'success': False,
            'error': str(e)
        }


def format_trending_content(trending_content: Dict[str, Any]) -> str:
    """
    å°†çƒ­é—¨å†…å®¹æ ¼å¼åŒ–ä¸ºå¯è¯»å­—ç¬¦ä¸²
    
    æ ¹æ®åŒºåŸŸè‡ªåŠ¨æ ¼å¼åŒ–ï¼š
    - ä¸­æ–‡åŒºåŸŸï¼šBç«™å’Œå¾®åšå†…å®¹ï¼Œä¸­æ–‡æ˜¾ç¤º
    - éä¸­æ–‡åŒºåŸŸï¼šRedditå’ŒTwitterå†…å®¹ï¼Œè‹±æ–‡æ˜¾ç¤º
    
    Args:
        trending_content: fetch_trending_contentè¿”å›çš„ç»“æœ
    
    Returns:
        æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
    """
    output_lines = []
    region = trending_content.get('region', 'china')
    
    if region == 'china':
        # æ ¼å¼åŒ–Bç«™å†…å®¹ï¼ˆä¸­æ–‡ï¼‰
        bilibili_data = trending_content.get('bilibili', {})
        if bilibili_data.get('success'):
            output_lines.append("ã€Bç«™é¦–é¡µæ¨èã€‘")
            videos = bilibili_data.get('videos', [])
            
            for i, video in enumerate(videos[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                title = video.get('title', '')
                author = video.get('author', '')
                url = video.get('url', '')
                
                output_lines.append(f"{i}. {title}")
                output_lines.append(f"   UPä¸»: {author}")
                if url:
                    output_lines.append(f"   é“¾æ¥: {url}")
            
            output_lines.append("")  # ç©ºè¡Œ
        
        # æ ¼å¼åŒ–å¾®åšå†…å®¹ï¼ˆä¸­æ–‡ï¼‰
        weibo_data = trending_content.get('weibo', {})
        if weibo_data.get('success'):
            output_lines.append("ã€å¾®åšçƒ­è®®è¯é¢˜ã€‘")
            trending_list = weibo_data.get('trending', [])
            
            for i, item in enumerate(trending_list[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                word = item.get('word', '')
                note = item.get('note', '')
                url = item.get('url', '')
                
                line = f"{i}. {word}"
                if note:
                    line += f" [{note}]"
                output_lines.append(line)
                if url:
                    output_lines.append(f"   é“¾æ¥: {url}")
        
        if not output_lines:
            return "æš‚æ—¶æ— æ³•è·å–æ¨èå†…å®¹"
    else:
        # æ ¼å¼åŒ–Redditå†…å®¹ï¼ˆè‹±æ–‡ï¼‰
        reddit_data = trending_content.get('reddit', {})
        if reddit_data.get('success'):
            output_lines.append("ã€Reddit Hot Postsã€‘")
            posts = reddit_data.get('posts', [])
            
            for i, post in enumerate(posts[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                title = post.get('title', '')
                subreddit = post.get('subreddit', '')
                score = post.get('score', '')
                url = post.get('url', '')
                
                output_lines.append(f"{i}. {title}")
                if subreddit:
                    output_lines.append(f"   {subreddit} | {score} upvotes")
                if url:
                    output_lines.append(f"   Link: {url}")
            
            output_lines.append("")  # ç©ºè¡Œ
        
        # æ ¼å¼åŒ–Twitterå†…å®¹ï¼ˆè‹±æ–‡ï¼‰
        twitter_data = trending_content.get('twitter', {})
        if twitter_data.get('success'):
            output_lines.append("ã€Twitter Trending Topicsã€‘")
            trending_list = twitter_data.get('trending', [])
            
            for i, item in enumerate(trending_list[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                word = item.get('word', '')
                tweet_count = item.get('tweet_count', '')
                note = item.get('note', '')
                url = item.get('url', '')
                
                line = f"{i}. {word}"
                if tweet_count and tweet_count != 'N/A':
                    line += f" ({tweet_count} tweets)"
                if note:
                    line += f" - {note}"
                output_lines.append(line)
                if url:
                    output_lines.append(f"   Link: {url}")
        
        if not output_lines:
            return "æš‚æ—¶æ— æ³•è·å–çƒ­é—¨å†…å®¹"
    
    return "\n".join(output_lines)


def get_active_window_title(include_raw: bool = False) -> Optional[Union[str, Dict[str, str]]]:
    """
    è·å–å½“å‰æ´»è·ƒçª—å£çš„æ ‡é¢˜ï¼ˆä»…æ”¯æŒWindowsï¼‰
    
    Args:
        include_raw: æ˜¯å¦è¿”å›åŸå§‹æ ‡é¢˜ã€‚é»˜è®¤Falseï¼Œä»…è¿”å›æˆªæ–­åçš„å®‰å…¨æ ‡é¢˜ã€‚
                     è®¾ä¸ºTrueæ—¶è¿”å›åŒ…å«sanitizedå’Œrawçš„å­—å…¸ã€‚
    
    Returns:
        é»˜è®¤æƒ…å†µï¼šæˆªæ–­åçš„å®‰å…¨æ ‡é¢˜å­—ç¬¦ä¸²ï¼ˆå‰30å­—ç¬¦ï¼‰ï¼Œå¤±è´¥è¿”å›None
        include_raw=Trueæ—¶ï¼š{'sanitized': 'æˆªæ–­æ ‡é¢˜', 'raw': 'å®Œæ•´æ ‡é¢˜'}ï¼Œå¤±è´¥è¿”å›None
    """
    if platform.system() != 'Windows':
        logger.warning("è·å–æ´»è·ƒçª—å£æ ‡é¢˜ä»…æ”¯æŒWindowsç³»ç»Ÿ")
        return None
    
    try:
        import pygetwindow as gw
    except ImportError:
        logger.error("pygetwindowæ¨¡å—æœªå®‰è£…ã€‚åœ¨Windowsç³»ç»Ÿä¸Šè¯·å®‰è£…: pip install pygetwindow")
        return None
    
    try:
        active_window = gw.getActiveWindow()
        if active_window:
            raw_title = active_window.title
            # æˆªæ–­æ ‡é¢˜ä»¥é¿å…è®°å½•æ•æ„Ÿä¿¡æ¯
            sanitized_title = raw_title[:30] + '...' if len(raw_title) > 30 else raw_title
            logger.info(f"è·å–åˆ°æ´»è·ƒçª—å£æ ‡é¢˜: {sanitized_title}")
            
            if include_raw:
                return {
                    'sanitized': sanitized_title,
                    'raw': raw_title
                }
            else:
                return sanitized_title
        else:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçª—å£")
            return None
    except Exception as e:
        logger.exception(f"è·å–æ´»è·ƒçª—å£æ ‡é¢˜å¤±è´¥: {e}")
        return None


async def generate_diverse_queries(window_title: str) -> List[str]:
    """
    ä½¿ç”¨LLMåŸºäºçª—å£æ ‡é¢˜ç”Ÿæˆ3ä¸ªå¤šæ ·åŒ–çš„æœç´¢å…³é”®è¯
    
    æ ¹æ®ç”¨æˆ·åŒºåŸŸè‡ªåŠ¨ä½¿ç”¨é€‚å½“çš„è¯­è¨€ï¼š
    - ä¸­æ–‡åŒºåŸŸï¼šä¸­æ–‡æç¤ºè¯ï¼Œç”¨äºç™¾åº¦æœç´¢
    - éä¸­æ–‡åŒºåŸŸï¼šè‹±æ–‡æç¤ºè¯ï¼Œç”¨äºGoogleæœç´¢
    
    Args:
        window_title: çª—å£æ ‡é¢˜ï¼ˆåº”è¯¥æ˜¯å·²æ¸…ç†çš„æ ‡é¢˜ï¼Œä¸åº”åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼‰
    
    Returns:
        åŒ…å«3ä¸ªæœç´¢å…³é”®è¯çš„åˆ—è¡¨
    
    æ³¨æ„ï¼š
        ä¸ºä¿æŠ¤éšç§ï¼Œè°ƒç”¨æ­¤å‡½æ•°å‰åº”å…ˆä½¿ç”¨clean_window_title()æ¸…ç†æ ‡é¢˜ï¼Œ
        é¿å…å°†æ–‡ä»¶è·¯å¾„ã€è´¦å·ç­‰æ•æ„Ÿä¿¡æ¯å‘é€ç»™LLM API
    """
    try:
        # å¯¼å…¥é…ç½®ç®¡ç†å™¨
        from utils.config_manager import ConfigManager
        config_manager = ConfigManager()
        
        # ä½¿ç”¨correctionæ¨¡å‹é…ç½®ï¼ˆè½»é‡çº§æ¨¡å‹ï¼Œé€‚åˆæ­¤ä»»åŠ¡ï¼‰
        correction_config = config_manager.get_model_api_config('correction')
        
        llm = ChatOpenAI(
            model=correction_config['model'],
            base_url=correction_config['base_url'],
            api_key=correction_config['api_key'],
            temperature=1.0,  # æé«˜temperatureä»¥è·å¾—æ›´å¤šæ ·åŒ–çš„ç»“æœ
            timeout=10.0
        )
        
        # æ¸…ç†/è„±æ•çª—å£æ ‡é¢˜ç”¨äºæ—¥å¿—æ˜¾ç¤º
        sanitized_title = window_title[:30] + '...' if len(window_title) > 30 else window_title
        
        # æ£€æµ‹åŒºåŸŸå¹¶ä½¿ç”¨é€‚å½“çš„æç¤ºè¯
        china_region = is_china_region()
        
        if china_region:
            prompt = f"""åŸºäºä»¥ä¸‹çª—å£æ ‡é¢˜ï¼Œç”Ÿæˆ3ä¸ªä¸åŒçš„æœç´¢å…³é”®è¯ï¼Œç”¨äºåœ¨ç™¾åº¦ä¸Šæœç´¢ç›¸å…³å†…å®¹ã€‚

çª—å£æ ‡é¢˜ï¼š{window_title}

è¦æ±‚ï¼š
1. ç”Ÿæˆ3ä¸ªä¸åŒè§’åº¦çš„æœç´¢å…³é”®è¯
2. å…³é”®è¯åº”è¯¥ç®€æ´ï¼ˆ2-8ä¸ªå­—ï¼‰
3. å…³é”®è¯åº”è¯¥å¤šæ ·åŒ–ï¼Œæ¶µç›–ä¸åŒæ–¹é¢
4. åªè¾“å‡º3ä¸ªå…³é”®è¯ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦æ·»åŠ ä»»ä½•åºå·ã€æ ‡ç‚¹æˆ–å…¶ä»–å†…å®¹

ç¤ºä¾‹è¾“å‡ºæ ¼å¼ï¼š
å…³é”®è¯1
å…³é”®è¯2
å…³é”®è¯3"""
        else:
            prompt = f"""Based on the following window title, generate 3 different search keywords for Google search.

Window title: {window_title}

Requirements:
1. Generate 3 keywords from different angles
2. Keywords should be concise (2-6 words each)
3. Keywords should be diverse, covering different aspects
4. Output only 3 keywords, one per line, without any numbers, punctuation, or other content

Example output format:
keyword one
keyword two
keyword three"""

        # ä½¿ç”¨å¼‚æ­¥è°ƒç”¨
        response = await llm.ainvoke([SystemMessage(content=prompt)])
        
        # è§£æå“åº”ï¼Œæå–3ä¸ªå…³é”®è¯
        queries = []
        lines = response.content.strip().split('\n')
        for line in lines:
            line = line.strip()
            # ç§»é™¤å¯èƒ½çš„åºå·ã€æ ‡ç‚¹ç­‰
            line = re.sub(r'^[\d\.\-\*\)\]ã€‘]+\s*', '', line)
            line = line.strip('.,;:ï¼Œã€‚ï¼›ï¼š')
            if line and len(line) >= 2:
                queries.append(line)
                if len(queries) >= 3:
                    break
        
        # å¦‚æœç”Ÿæˆçš„æŸ¥è¯¢ä¸è¶³3ä¸ªï¼Œç”¨åŸå§‹æ ‡é¢˜å¡«å……
        if len(queries) < 3:
            clean_title = clean_window_title(window_title)
            while len(queries) < 3 and clean_title:
                queries.append(clean_title)
        
        # ä½¿ç”¨è„±æ•åçš„æ ‡é¢˜è®°å½•æ—¥å¿—
        if china_region:
            logger.info(f"ä¸ºçª—å£æ ‡é¢˜ã€Œ{sanitized_title}ã€ç”Ÿæˆçš„æŸ¥è¯¢å…³é”®è¯: {queries}")
        else:
            logger.info(f"ä¸ºçª—å£æ ‡é¢˜ã€Œ{sanitized_title}ã€ç”Ÿæˆçš„æŸ¥è¯¢å…³é”®è¯: {queries}")
        return queries[:3]
        
    except Exception as e:
        # å¼‚å¸¸æ—¥å¿—ä¸­ä¹Ÿä½¿ç”¨è„±æ•æ ‡é¢˜
        sanitized_title = window_title[:30] + '...' if len(window_title) > 30 else window_title
        if is_china_region():
            logger.warning(f"ä¸ºçª—å£æ ‡é¢˜ã€Œ{sanitized_title}ã€ç”Ÿæˆå¤šæ ·åŒ–æŸ¥è¯¢å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¸…ç†æ–¹æ³•: {e}")
        else:
            logger.warning(f"ä¸ºçª—å£æ ‡é¢˜ã€Œ{sanitized_title}ã€ç”Ÿæˆå¤šæ ·åŒ–æŸ¥è¯¢å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¸…ç†æ–¹æ³•: {e}")
        # å›é€€åˆ°åŸå§‹æ¸…ç†æ–¹æ³•
        clean_title = clean_window_title(window_title)
        return [clean_title, clean_title, clean_title]


def clean_window_title(title: str) -> str:
    """
    æ¸…ç†çª—å£æ ‡é¢˜ï¼Œæå–æœ‰æ„ä¹‰çš„æœç´¢å…³é”®è¯
    
    Args:
        title: åŸå§‹çª—å£æ ‡é¢˜
    
    Returns:
        æ¸…ç†åçš„æœç´¢å…³é”®è¯
    """
    if not title:
        return ""
    
    # ç§»é™¤å¸¸è§çš„åº”ç”¨ç¨‹åºåç¼€å’Œæ— æ„ä¹‰å†…å®¹
    patterns_to_remove = [
        r'\s*[-â€“â€”]\s*(Google Chrome|Mozilla Firefox|Microsoft Edge|Opera|Safari|Brave).*$',
        r'\s*[-â€“â€”]\s*(Visual Studio Code|VS Code|VSCode).*$',
        r'\s*[-â€“â€”]\s*(è®°äº‹æœ¬|Notepad\+*|Sublime Text|Atom).*$',
        r'\s*[-â€“â€”]\s*(Microsoft Word|Excel|PowerPoint).*$',
        r'\s*[-â€“â€”]\s*(QQéŸ³ä¹|ç½‘æ˜“äº‘éŸ³ä¹|é…·ç‹—éŸ³ä¹|Spotify).*$',
        r'\s*[-â€“â€”]\s*(å“”å“©å“”å“©|bilibili|YouTube|ä¼˜é…·|çˆ±å¥‡è‰º|è…¾è®¯è§†é¢‘).*$',
        r'\s*[-â€“â€”]\s*\d+\s*$',  # ç§»é™¤æœ«å°¾çš„æ•°å­—ï¼ˆå¦‚é¡µç ï¼‰
        r'^\*\s*',  # ç§»é™¤å¼€å¤´çš„æ˜Ÿå·ï¼ˆæœªä¿å­˜æ ‡è®°ï¼‰
        r'\s*\[.*?\]\s*$',  # ç§»é™¤æ–¹æ‹¬å·å†…å®¹
        r'\s*\(.*?\)\s*$',  # ç§»é™¤åœ†æ‹¬å·å†…å®¹
        r'https?://\S+',  # ç§»é™¤URL
        r'www\.\S+',  # ç§»é™¤wwwå¼€å¤´çš„ç½‘å€
        r'\.py\s*$',  # ç§»é™¤.pyåç¼€
        r'\.js\s*$',  # ç§»é™¤.jsåç¼€
        r'\.html?\s*$',  # ç§»é™¤.htmlåç¼€
        r'\.css\s*$',  # ç§»é™¤.cssåç¼€
        r'\.md\s*$',  # ç§»é™¤.mdåç¼€
        r'\.txt\s*$',  # ç§»é™¤.txtåç¼€
        r'\.json\s*$',  # ç§»é™¤.jsonåç¼€
    ]
    
    cleaned = title
    for pattern in patterns_to_remove:
        cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
    
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    cleaned = ' '.join(cleaned.split())
    
    # å¦‚æœæ¸…ç†åå¤ªçŸ­æˆ–ä¸ºç©ºï¼Œè¿”å›åŸæ ‡é¢˜çš„ä¸€éƒ¨åˆ†
    if len(cleaned) < 3:
        # å°è¯•æå–åŸæ ‡é¢˜ä¸­çš„ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„éƒ¨åˆ†
        parts = re.split(r'\s*[-â€“â€”|]\s*', title)
        if parts and len(parts[0]) >= 3:
            cleaned = parts[0].strip()
    
    return cleaned[:100]  # é™åˆ¶é•¿åº¦


async def search_google(query: str, limit: int = 10) -> Dict[str, Any]:
    """
    ä½¿ç”¨Googleæœç´¢å…³é”®è¯å¹¶è·å–æœç´¢ç»“æœï¼ˆç”¨äºéä¸­æ–‡åŒºåŸŸï¼‰
    
    Args:
        query: æœç´¢å…³é”®è¯
        limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
    
    Returns:
        åŒ…å«æœç´¢ç»“æœçš„å­—å…¸
    """
    try:
        if not query or len(query.strip()) < 2:
            return {
                'success': False,
                'error': 'æœç´¢å…³é”®è¯å¤ªçŸ­'
            }
        
        # æ¸…ç†æŸ¥è¯¢è¯
        query = query.strip()
        encoded_query = quote(query)
        
        # Googleæœç´¢URL
        url = f"https://www.google.com/search?q={encoded_query}&hl=en"
        
        headers = {
            'User-Agent': get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Connection': 'keep-alive',
            'DNT': '1',
            'Cache-Control': 'no-cache',
        }
        
        # æ·»åŠ éšæœºå»¶è¿Ÿ
        await asyncio.sleep(random.uniform(0.2, 0.5))
        
        async with httpx.AsyncClient(timeout=5.0, follow_redirects=True) as client:
            response = await client.get(url, headers=headers)
            response.raise_for_status()
            html_content = response.text
            
            # è§£ææœç´¢ç»“æœ
            results = parse_google_results(html_content, limit)
            
            if results:
                return {
                    'success': True,
                    'query': query,
                    'results': results
                }
            else:
                return {
                    'success': False,
                    'error': 'æœªèƒ½è§£æåˆ°æœç´¢ç»“æœ',
                    'query': query
                }
                
    except httpx.TimeoutException:
        logger.exception("Googleæœç´¢è¶…æ—¶")
        return {
            'success': False,
            'error': 'æœç´¢è¶…æ—¶'
        }
    except Exception as e:
        logger.exception(f"Googleæœç´¢å¤±è´¥: {e}")
        return {
            'success': False,
            'error': str(e)
        }


def parse_google_results(html_content: str, limit: int = 5) -> List[Dict[str, str]]:
    """
    è§£æGoogleæœç´¢ç»“æœé¡µé¢
    
    Args:
        html_content: HTMLé¡µé¢å†…å®¹
        limit: ç»“æœæ•°é‡é™åˆ¶
    
    Returns:
        æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å« title, abstract, url
    """
    results = []
    
    try:
        from urllib.parse import urljoin, urlparse, parse_qs
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æŸ¥æ‰¾æœç´¢ç»“æœå®¹å™¨
        # Googleä½¿ç”¨å„ç§ç±»åï¼Œå°è¯•å¤šä¸ªé€‰æ‹©å™¨
        result_divs = soup.find_all('div', class_='g')
        
        for div in result_divs[:limit * 2]:
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            link = div.find('a')
            if link:
                # è·å–h3æ ‡ç­¾ä½œä¸ºæ ‡é¢˜
                h3 = div.find('h3')
                if h3:
                    title = h3.get_text(strip=True)
                else:
                    title = link.get_text(strip=True)
                
                if title and 3 < len(title) < 200:
                    # æå–URL
                    href = link.get('href', '')
                    if href:
                        # Googleæœ‰æ—¶ä¼šåŒ…è£…URL
                        if href.startswith('/url?'):
                            parsed = urlparse(href)
                            qs = parse_qs(parsed.query)
                            url = qs.get('q', [href])[0]
                        elif href.startswith('http'):
                            url = href
                        else:
                            url = urljoin('https://www.google.com', href)
                    else:
                        url = ''
                    
                    # æå–æ‘˜è¦/ç‰‡æ®µ
                    abstract = ""
                    # æŸ¥æ‰¾ç‰‡æ®µæ–‡æœ¬
                    snippet_div = div.find('div', class_=lambda x: x and ('VwiC3b' in x if x else False))
                    if snippet_div:
                        abstract = snippet_div.get_text(strip=True)[:200]
                    else:
                        # å°è¯•å…¶ä»–å¸¸è§çš„ç‰‡æ®µé€‰æ‹©å™¨
                        spans = div.find_all('span')
                        for span in spans:
                            text = span.get_text(strip=True)
                            if len(text) > 50:
                                abstract = text[:200]
                                break
                    
                    # è·³è¿‡å¹¿å‘Šå’Œä¸éœ€è¦çš„ç»“æœ
                    if not any(skip in title.lower() for skip in ['ad', 'sponsored', 'javascript']):
                        results.append({
                            'title': title,
                            'abstract': abstract,
                            'url': url
                        })
                        if len(results) >= limit:
                            break
        
        logger.info(f"è§£æåˆ° {len(results)} æ¡Googleæœç´¢ç»“æœ")
        return results[:limit]
        
    except Exception as e:
        logger.exception(f"è§£æGoogleæœç´¢ç»“æœå¤±è´¥: {e}")
        return []


async def search_baidu(query: str, limit: int = 5) -> Dict[str, Any]:
    """
    ä½¿ç”¨ç™¾åº¦æœç´¢å…³é”®è¯å¹¶è·å–æœç´¢ç»“æœ
    
    Args:
        query: æœç´¢å…³é”®è¯
        limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
    
    Returns:
        åŒ…å«æœç´¢ç»“æœçš„å­—å…¸
    """
    try:
        if not query or len(query.strip()) < 2:
            return {
                'success': False,
                'error': 'æœç´¢å…³é”®è¯å¤ªçŸ­'
            }
        
        # æ¸…ç†æŸ¥è¯¢è¯
        query = query.strip()
        encoded_query = quote(query)
        
        # ç™¾åº¦æœç´¢URL
        url = f"https://www.baidu.com/s?wd={encoded_query}"
        
        headers = {
            'User-Agent': get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
            'Referer': 'https://www.baidu.com/',
            'DNT': '1',
            'Cache-Control': 'no-cache',
        }
        
        # æ·»åŠ éšæœºå»¶è¿Ÿ
        await asyncio.sleep(random.uniform(0.2, 0.5))
        
        async with httpx.AsyncClient(timeout=5.0, follow_redirects=True) as client:
            response = await client.get(url, headers=headers)
            response.raise_for_status()
            html_content = response.text
            
            # è§£ææœç´¢ç»“æœ
            results = parse_baidu_results(html_content, limit)
            
            if results:
                return {
                    'success': True,
                    'query': query,
                    'results': results
                }
            else:
                return {
                    'success': False,
                    'error': 'æœªèƒ½è§£æåˆ°æœç´¢ç»“æœ',
                    'query': query
                }
                
    except httpx.TimeoutException:
        logger.exception("ç™¾åº¦æœç´¢è¶…æ—¶")
        return {
            'success': False,
            'error': 'æœç´¢è¶…æ—¶'
        }
    except Exception as e:
        logger.exception(f"ç™¾åº¦æœç´¢å¤±è´¥: {e}")
        return {
            'success': False,
            'error': str(e)
        }


def parse_baidu_results(html_content: str, limit: int = 5) -> List[Dict[str, str]]:
    """
    è§£æç™¾åº¦æœç´¢ç»“æœé¡µé¢
    
    Args:
        html_content: HTMLé¡µé¢å†…å®¹
        limit: ç»“æœæ•°é‡é™åˆ¶
    
    Returns:
        æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å« title, abstract, url
    """
    results = []
    
    try:
        from urllib.parse import urljoin
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æå–æœç´¢ç»“æœå®¹å™¨
        containers = soup.find_all('div', class_=lambda x: x and 'c-container' in x, limit=limit * 2)
        
        for container in containers:
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            link = container.find('a')
            if link:
                title = link.get_text(strip=True)
                if title and 5 < len(title) < 200:
                    # æå– URLï¼ˆå¤„ç†ç›¸å¯¹å’Œç»å¯¹ URLï¼‰
                    href = link.get('href', '')
                    if href:
                        # å¦‚æœæ˜¯ç›¸å¯¹ URLï¼Œè½¬æ¢ä¸ºç»å¯¹ URL
                        if href.startswith('/'):
                            url = urljoin('https://www.baidu.com', href)
                        elif not href.startswith('http'):
                            url = urljoin('https://www.baidu.com/', href)
                        else:
                            url = href
                    else:
                        url = ''
                    
                    # æå–æ‘˜è¦
                    abstract = ""
                    content_span = container.find('span', class_=lambda x: x and 'content-right' in x)
                    if content_span:
                        abstract = content_span.get_text(strip=True)[:200]
                    
                    if not any(skip in title.lower() for skip in ['ç™¾åº¦', 'å¹¿å‘Š', 'javascript']):
                        results.append({
                            'title': title,
                            'abstract': abstract,
                            'url': url
                        })
                        if len(results) >= limit:
                            break
        
        # å¦‚æœæ²¡æ‰¾åˆ°ç»“æœï¼Œå°è¯•æå– h3 æ ‡é¢˜
        if not results:
            h3_links = soup.find_all('h3')
            for h3 in h3_links[:limit]:
                link = h3.find('a')
                if link:
                    title = link.get_text(strip=True)
                    if title and 5 < len(title) < 200:
                        # æå– URL
                        href = link.get('href', '')
                        if href:
                            if href.startswith('/'):
                                url = urljoin('https://www.baidu.com', href)
                            elif not href.startswith('http'):
                                url = urljoin('https://www.baidu.com/', href)
                            else:
                                url = href
                        else:
                            url = ''
                        
                        results.append({
                            'title': title,
                            'abstract': '',
                            'url': url
                        })
        
        logger.info(f"è§£æåˆ° {len(results)} æ¡ç™¾åº¦æœç´¢ç»“æœ")
        return results[:limit]
        
    except Exception as e:
        logger.exception(f"è§£æç™¾åº¦æœç´¢ç»“æœå¤±è´¥: {e}")
        return []


def format_baidu_search_results(search_result: Dict[str, Any]) -> str:
    """
    æ ¼å¼åŒ–ç™¾åº¦æœç´¢ç»“æœä¸ºå¯è¯»å­—ç¬¦ä¸²
    
    Args:
        search_result: search_baiduè¿”å›çš„ç»“æœ
    
    Returns:
        æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
    """
    if not search_result.get('success'):
        return f"æœç´¢å¤±è´¥: {search_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
    
    output_lines = []
    query = search_result.get('query', '')
    results = search_result.get('results', [])
    
    output_lines.append(f"ã€å…³äºã€Œ{query}ã€çš„æœç´¢ç»“æœã€‘")
    output_lines.append("")
    
    for i, result in enumerate(results, 1):
        title = result.get('title', '')
        abstract = result.get('abstract', '')
        
        output_lines.append(f"{i}. {title}")
        if abstract:
            # é™åˆ¶æ‘˜è¦é•¿åº¦
            abstract = abstract[:150] + '...' if len(abstract) > 150 else abstract
            output_lines.append(f"   {abstract}")
        output_lines.append("")
    
    if not results:
        output_lines.append("æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
    
    return "\n".join(output_lines)


def format_search_results(search_result: Dict[str, Any]) -> str:
    """
    å°†æœç´¢ç»“æœæ ¼å¼åŒ–ä¸ºå¯è¯»å­—ç¬¦ä¸²
    æ ¹æ®åŒºåŸŸè‡ªåŠ¨ä½¿ç”¨é€‚å½“çš„è¯­è¨€
    
    Args:
        search_result: search_baiduæˆ–search_googleè¿”å›çš„ç»“æœ
    
    Returns:
        æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
    """
    china_region = is_china_region()
    
    if not search_result.get('success'):
        if china_region:
            return f"æœç´¢å¤±è´¥: {search_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
        else:
            return f"Search failed: {search_result.get('error', 'Unknown error')}"
    
    output_lines = []
    query = search_result.get('query', '')
    results = search_result.get('results', [])
    
    if china_region:
        output_lines.append(f"ã€å…³äºã€Œ{query}ã€çš„æœç´¢ç»“æœã€‘")
    else:
        output_lines.append(f"ã€Search results forã€Œ{query}ã€ã€‘")
    output_lines.append("")
    
    for i, result in enumerate(results, 1):
        title = result.get('title', '')
        abstract = result.get('abstract', '')
        
        output_lines.append(f"{i}. {title}")
        if abstract:
            abstract = abstract[:150] + '...' if len(abstract) > 150 else abstract
            output_lines.append(f"   {abstract}")
        output_lines.append("")
    
    if not results:
        if china_region:
            output_lines.append("æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
        else:
            output_lines.append("No results found")
    
    return "\n".join(output_lines)


async def fetch_window_context_content(limit: int = 5) -> Dict[str, Any]:
    """
    è·å–å½“å‰æ´»è·ƒçª—å£æ ‡é¢˜å¹¶è¿›è¡Œæœç´¢
    
    ä½¿ç”¨åŒºåŸŸæ£€æµ‹æ¥å†³å®šæœç´¢å¼•æ“ï¼š
    - ä¸­æ–‡åŒºåŸŸï¼šç™¾åº¦æœç´¢
    - éä¸­æ–‡åŒºåŸŸï¼šGoogleæœç´¢
    
    Args:
        limit: æœç´¢ç»“æœæ•°é‡é™åˆ¶
    
    Returns:
        åŒ…å«çª—å£æ ‡é¢˜å’Œæœç´¢ç»“æœçš„å­—å…¸
        æ³¨æ„ï¼šwindow_titleæ˜¯è„±æ•åçš„ç‰ˆæœ¬ä»¥ä¿æŠ¤éšç§
    """
    try:
        # æ£€æµ‹åŒºåŸŸ
        china_region = is_china_region()
        
        # è·å–æ´»è·ƒçª—å£æ ‡é¢˜ï¼ˆåŒæ—¶è·å–åŸå§‹å’Œè„±æ•ç‰ˆæœ¬ï¼‰
        title_result = get_active_window_title(include_raw=True)
        
        if not title_result:
            if china_region:
                return {
                    'success': False,
                    'error': 'æ— æ³•è·å–å½“å‰æ´»è·ƒçª—å£æ ‡é¢˜'
                }
            else:
                return {
                    'success': False,
                    'error': 'æ— æ³•è·å–å½“å‰æ´»è·ƒçª—å£æ ‡é¢˜'
                }
        
        sanitized_title = title_result['sanitized']
        raw_title = title_result['raw']
        
        # æ¸…ç†çª—å£æ ‡é¢˜ä»¥ç§»é™¤æ•æ„Ÿä¿¡æ¯ï¼Œé¿å…å‘é€ç»™LLM
        cleaned_title = clean_window_title(raw_title)
        
        # ä½¿ç”¨æ¸…ç†åçš„æ ‡é¢˜ç”Ÿæˆå¤šæ ·åŒ–æœç´¢æŸ¥è¯¢ï¼ˆä¿æŠ¤éšç§ï¼‰
        search_queries = await generate_diverse_queries(cleaned_title)
        
        if not search_queries or all(not q or len(q) < 2 for q in search_queries):
            if china_region:
                return {
                    'success': False,
                    'error': 'çª—å£æ ‡é¢˜æ— æ³•æå–æœ‰æ•ˆçš„æœç´¢å…³é”®è¯',
                    'window_title': sanitized_title
                }
            else:
                return {
                    'success': False,
                    'error': 'çª—å£æ ‡é¢˜æ— æ³•æå–æœ‰æ•ˆçš„æœç´¢å…³é”®è¯',
                    'window_title': sanitized_title
                }
        
        # æ—¥å¿—ä¸­ä½¿ç”¨è„±æ•åçš„æ ‡é¢˜
        if china_region:
            logger.info(f"ä»çª—å£æ ‡é¢˜ã€Œ{sanitized_title}ã€ç”Ÿæˆå¤šæ ·åŒ–æŸ¥è¯¢: {search_queries}")
        else:
            logger.info(f"ä»çª—å£æ ‡é¢˜ã€Œ{sanitized_title}ã€ç”Ÿæˆå¤šæ ·åŒ–æŸ¥è¯¢: {search_queries}")
        
        # æ‰§è¡Œæœç´¢å¹¶åˆå¹¶ç»“æœ
        all_results = []
        successful_queries = []
        
        # æ ¹æ®åŒºåŸŸé€‰æ‹©æœç´¢å‡½æ•°
        search_func = search_baidu if china_region else search_google
        
        for query in search_queries:
            if not query or len(query) < 2:
                continue
            
            if china_region:
                logger.info(f"ä½¿ç”¨æŸ¥è¯¢å…³é”®è¯: {query}")
            else:
                logger.info(f"ä½¿ç”¨æŸ¥è¯¢å…³é”®è¯: {query}")
            
            search_result = await search_func(query, limit)
            
            if search_result.get('success') and search_result.get('results'):
                all_results.extend(search_result['results'])
                successful_queries.append(query)
        
        # å»é‡ç»“æœï¼ˆä¼˜å…ˆä½¿ç”¨URLï¼Œå¦‚æœURLç¼ºå¤±åˆ™ä½¿ç”¨titleï¼‰
        seen_keys = set()
        unique_results = []
        for result in all_results:
            url = result.get('url', '')
            title = result.get('title', '')
            
            # ä¼˜å…ˆä½¿ç”¨URLè¿›è¡Œå»é‡ï¼Œå›é€€åˆ°title
            dedup_key = url if url else title
            
            if dedup_key and dedup_key not in seen_keys:
                seen_keys.add(dedup_key)
                unique_results.append(result)
        
        # é™åˆ¶æ€»ç»“æœæ•°é‡
        unique_results = unique_results[:limit * 2]
        
        if not unique_results:
            if china_region:
                return {
                    'success': False,
                    'error': 'æ‰€æœ‰æŸ¥è¯¢å‡æœªè·å¾—æœç´¢ç»“æœ',
                    'window_title': sanitized_title,
                    'search_queries': search_queries
                }
            else:
                return {
                    'success': False,
                    'error': 'æ‰€æœ‰æŸ¥è¯¢å‡æœªè·å¾—æœç´¢ç»“æœ',
                    'window_title': sanitized_title,
                    'search_queries': search_queries
                }
        
        return {
            'success': True,
            'window_title': sanitized_title,
            'search_queries': successful_queries,
            'search_results': unique_results,
            'region': 'china' if china_region else 'non-china'
        }
        
    except Exception as e:
        if is_china_region():
            logger.exception(f"è·å–çª—å£ä¸Šä¸‹æ–‡å†…å®¹å¤±è´¥: {e}")
        else:
            logger.exception(f"è·å–çª—å£ä¸Šä¸‹æ–‡å†…å®¹å¤±è´¥: {e}")
        return {
            'success': False,
            'error': str(e)
        }


def format_window_context_content(content: Dict[str, Any]) -> str:
    """
    å°†çª—å£ä¸Šä¸‹æ–‡å†…å®¹æ ¼å¼åŒ–ä¸ºå¯è¯»å­—ç¬¦ä¸²
    
    æ ¹æ®åŒºåŸŸè‡ªåŠ¨ä½¿ç”¨é€‚å½“çš„è¯­è¨€
    
    Args:
        content: fetch_window_context_contentè¿”å›çš„ç»“æœ
    
    Returns:
        æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
    """
    china_region = is_china_region()
    
    if not content.get('success'):
        if china_region:
            return f"è·å–çª—å£ä¸Šä¸‹æ–‡å¤±è´¥: {content.get('error', 'æœªçŸ¥é”™è¯¯')}"
        else:
            return f"Failed to fetch window context: {content.get('error', 'Unknown error')}"
    
    output_lines = []
    window_title = content.get('window_title', '')
    search_queries = content.get('search_queries', [])
    results = content.get('search_results', [])
    
    if china_region:
        output_lines.append(f"ã€å½“å‰æ´»è·ƒçª—å£ã€‘{window_title}")
        
        if search_queries:
            if len(search_queries) == 1:
                output_lines.append(f"ã€æœç´¢å…³é”®è¯ã€‘{search_queries[0]}")
            else:
                output_lines.append(f"ã€æœç´¢å…³é”®è¯ã€‘{', '.join(search_queries)}")
        
        output_lines.append("")
        output_lines.append("ã€ç›¸å…³ä¿¡æ¯ã€‘")
    else:
        output_lines.append(f"ã€Active Windowã€‘{window_title}")
        
        if search_queries:
            if len(search_queries) == 1:
                output_lines.append(f"ã€Search Keywordsã€‘{search_queries[0]}")
            else:
                output_lines.append(f"ã€Search Keywordsã€‘{', '.join(search_queries)}")
        
        output_lines.append("")
        output_lines.append("ã€Related Informationã€‘")
    
    for i, result in enumerate(results, 1):
        title = result.get('title', '')
        abstract = result.get('abstract', '')
        url = result.get('url', '')
        
        output_lines.append(f"{i}. {title}")
        if abstract:
            abstract = abstract[:150] + '...' if len(abstract) > 150 else abstract
            output_lines.append(f"   {abstract}")
        if url:
            if china_region:
                output_lines.append(f"   é“¾æ¥: {url}")
            else:
                output_lines.append(f"   Link: {url}")
    
    if not results:
        if china_region:
            output_lines.append("æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
        else:
            output_lines.append("No related information found")
    
    return "\n".join(output_lines)


# æµ‹è¯•ç”¨çš„ä¸»å‡½æ•°
async def main():
    """
    Webçˆ¬è™«çš„æµ‹è¯•å‡½æ•°
    è‡ªåŠ¨æ£€æµ‹åŒºåŸŸå¹¶è·å–ç›¸åº”å†…å®¹
    """
    china_region = is_china_region()
    
    if china_region:
        print("æ£€æµ‹åˆ°ä¸­æ–‡åŒºåŸŸ")
        print("æ­£åœ¨è·å–çƒ­é—¨å†…å®¹ï¼ˆBç«™ã€å¾®åšï¼‰...")
    else:
        print("æ£€æµ‹åˆ°éä¸­æ–‡åŒºåŸŸ")
        print("æ­£åœ¨è·å–çƒ­é—¨å†…å®¹ï¼ˆRedditã€Twitterï¼‰...")
    
    content = await fetch_trending_content(
        bilibili_limit=5, 
        weibo_limit=5,
        reddit_limit=5,
        twitter_limit=5
    )
    
    if content['success']:
        formatted = format_trending_content(content)
        print("\n" + "="*50)
        print(formatted)
        print("="*50)
    else:
        if china_region:
            print(f"è·å–å¤±è´¥: {content.get('error')}")
        else:
            print(f"è·å–å¤±è´¥: {content.get('error')}")


if __name__ == "__main__":
    asyncio.run(main())
