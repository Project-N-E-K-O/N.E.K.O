import asyncio
import websockets
import json
import logging
import torch
import sys
import os
import time
import threading
import uuid
import queue
import numpy as np
import re
from pathlib import Path

# ========================================================
# 1. ÂàùÂßãÂåñ Logging
# ========================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [TTS-Server] %(levelname)s - %(message)s'
)
logger = logging.getLogger("Qwen3-TTS-Server")

# ========================================================
# 2. Ë∑ØÂæÑÈÖçÁΩÆ (ÈÄÇÈÖç Linux ÂéüÁîüË∑ØÂæÑ)
# ========================================================
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
# ÂÅáËÆæ‰Ω†ËøòÂú® WSL ÁöÑËøô‰∏™‰ΩçÁΩÆ
MODEL_SOURCE_DIR = os.path.join(PROJECT_ROOT, "Qwen3-TTS")

if MODEL_SOURCE_DIR not in sys.path:
    sys.path.append(MODEL_SOURCE_DIR)

try:
    from qwen_tts.core.models.modeling_qwen3_tts import Qwen3TTSForConditionalGeneration
    from qwen_tts.core.models.processing_qwen3_tts import Qwen3TTSProcessor
    from qwen_tts.inference.qwen3_tts_model import Qwen3TTSModel, VoiceClonePromptItem

    torch.serialization.add_safe_globals([VoiceClonePromptItem])
    logger.info("‚úÖ ÊàêÂäüÂØºÂÖ• Qwen3-TTS ÂéüÁîüÁªÑ‰ª∂")
except ImportError as e:
    logger.error(f"‚ùå ÁªÑ‰ª∂ÂØºÂÖ•Â§±Ë¥•: {e}")
    sys.exit(1)


# ========================================================
# 3. Server Á±ªÂÆö‰πâ (ËûçÂêàÁâà)
# ========================================================
class QwenLocalServer:
    def __init__(
        self,
        model_path,
        voice_pt_path=None,
        ref_wav=None,
        ref_text=None,
        language=None,
        chunk_size=None,
        buffer_fallback_chars=None,
    ):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.pt_path = voice_pt_path or os.path.join(PROJECT_ROOT, "nyaning_voice.pt")
        self.model = None
        self.cached_prompt = None
        self.pad_token_id = None
        self.ref_text = ref_text or "„Ç¢„É©„Éê„Éû „Ç∑„É•„Éº „Éé „Çµ„Ç§„ÉÄ„Ç§ „Éà„Ç∑ „ÉØ „Éê„Éº„Éü„É≥„Ç∞„Éè„É† „Éá „Ç¢„É´„ÄÇ"
        self.ref_wav = ref_wav or os.path.join(PROJECT_ROOT, "uttid_f1.wav")
        self.language = language or "Chinese"
        self.chunk_size = int(chunk_size) if chunk_size else 4096
        self.buffer_fallback_chars = int(buffer_fallback_chars) if buffer_fallback_chars else 30
        self.inference_loc =threading.Lock()

        # --- ‰ªªÂä°ÈòüÂàó (Ëß£ÂÜ≥Âπ∂ÂèëÂç°È°ø) ---
        self.task_queue = queue.Queue()

        self._load_engine(model_path)
        # ÂêØÂä®ÂêéÂè∞Â∑•‰∫∫
        threading.Thread(target=self._worker_loop, daemon=True).start()

    def _load_engine(self, model_path):
        try:
            t0 = time.time()
            logger.info(f"Ê≠£Âú®ÂêØÂä® N.E.K.O ËØ≠Èü≥ÂºïÊìé (Device: {self.device})...")

            processor = Qwen3TTSProcessor.from_pretrained(model_path, fix_mistral_regex=True)

            # --- ÂÖ≥ÈîÆ‰ºòÂåñ 1: ÊòæÂºèÊåáÂÆö Bfloat16 Âíå Flash Attention 2 ---
            raw_model = Qwen3TTSForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,  # Áªô transformers Áúã
                dtype=torch.bfloat16,  # Áªô qwen Áúã
                attn_implementation="flash_attention_2",
                device_map="cuda",
                low_cpu_mem_usage=True
            )
            raw_model.eval()

            self.model = Qwen3TTSModel(model=raw_model, processor=processor)

            # --- ÂÖ≥ÈîÆ‰ºòÂåñ 2: È¢ÑËÆæ Config Èò≤Ê≠¢Êé®ÁêÜÊó∂ÂèçÂ§çÂàùÂßãÂåñ ---
            self.pad_token_id = raw_model.config.pad_token_id or raw_model.config.eos_token_id
            if hasattr(self.model, 'generation_config'):
                self.model.generation_config.pad_token_id = self.pad_token_id

            # Âä†ËΩΩ/ÊèêÂèñÈü≥Ëâ≤
            if os.path.exists(self.pt_path):
                logger.info(f"‚ú® ÂèëÁé∞Èü≥Ëâ≤ÁâπÂæÅ {self.pt_path}ÔºåÂä†ËΩΩ‰∏≠...")
                self.cached_prompt = torch.load(self.pt_path, map_location=self.device, weights_only=False)
            else:
                logger.warning("üéôÔ∏è Êú™ÂèëÁé∞Èü≥Ëâ≤ÁâπÂæÅÔºåÂ∞ùËØïÊèêÂèñ...")
                ref_wav = self.ref_wav
                if os.path.exists(ref_wav):
                    with torch.no_grad():
                        with torch.amp.autocast(self.device, dtype=torch.bfloat16):
                            self.cached_prompt = self.model.create_voice_clone_prompt(
                                ref_audio=ref_wav, ref_text=self.ref_text
                            )
                    torch.save(self.cached_prompt, self.pt_path)
                    logger.info("‚úÖ Èü≥Ëâ≤ÊèêÂèñÂÆåÊàê")
                else:
                    logger.error(f"‚ùå Êâæ‰∏çÂà∞ÂèÇËÄÉÈü≥È¢ë: {ref_wav}")

            logger.info(f"üöÄ ÂºïÊìéÂ∞±Áª™ | Á≤æÂ∫¶: {raw_model.dtype} | ËÄóÊó∂: {time.time() - t0:.2f}s")
        except Exception as e:
            logger.error(f"‚ùå Âä†ËΩΩÂºïÊìéÂºÇÂ∏∏: {e}")

    def _worker_loop(self):
        logger.info("üë∑ Êô∫ËÉΩÊãºÂè•ÈòüÂàóÊúçÂä°Â∑≤ÂêØÂä®")
        while True:
            task = self.task_queue.get()
            if task is None: break

            full_text, job_id, loop, audio_queue, cancel_event = task

            if cancel_event.is_set():
                self.task_queue.task_done()
                continue

            self._do_inference(full_text, job_id, loop, audio_queue, cancel_event)
            self.task_queue.task_done()

    def _do_inference(self, full_text, job_id, loop, audio_queue, cancel_event):
        try:
            if not self.model or self.cached_prompt is None: return

            start_time = time.time()

            # --- ÂÖ≥ÈîÆ‰ºòÂåñ 3: ‰ΩøÁî® inference_mode Âíå autocast ---
            with torch.inference_mode():
                with self.inference_lock, torch.amp.autocast(self.device, dtype=torch.bfloat16):
                    wavs, sr = self.model.generate_voice_clone(
                        text=full_text,
                        voice_clone_prompt=self.cached_prompt,
                        language=self.language,
                        pad_token_id=self.pad_token_id
                    )

            if torch.cuda.is_available():
                torch.cuda.synchronize()

            inference_duration = time.time() - start_time

            # --- üö® Ê†∏ÂøÉ‰øÆÂ§çÔºöÊâæÂõû‰∏¢Â§±ÁöÑ Int16 ËΩ¨Êç¢ (ËøôÂ∞±ÊòØÊ≤°Â£∞Èü≥ÁöÑÂéüÂõ†ÔºÅ) ---
            # ÂÖîËÄÅÂ∏àÁöÑ‰ª£Á†ÅÂèØËÉΩÁõ¥Êé•Âèë‰∫Ü floatÔºåÊàë‰ª¨ÈúÄË¶ÅËΩ¨Âõû int16
            audio_data = wavs[0].flatten()
            audio_int16 = (audio_data * 32767).astype(np.int16)

            audio_real_duration = len(audio_int16) / sr
            rtf = inference_duration / audio_real_duration if audio_real_duration > 0 else 0

            logger.info(
                f"‚úÖ [{job_id}] ÂÆåÊàê | ËÄóÊó∂:{inference_duration:.3f}s | Èü≥È¢ë:{audio_real_duration:.2f}s | RTF:{rtf:.4f}"
            )

            # ÂèëÈÄÅÊï∞ÊçÆ
            chunk_size = self.chunk_size
            for i in range(0, len(audio_int16), chunk_size):
                if cancel_event.is_set(): break
                chunk = audio_int16[i:i + chunk_size].tobytes()
                loop.call_soon_threadsafe(audio_queue.put_nowait, chunk)

        except Exception as e:
            logger.error(f"‚ùå Êé®ÁêÜÈîôËØØ: {e}")
            import traceback
            logger.error(traceback.format_exc())
        finally:
            loop.call_soon_threadsafe(audio_queue.put_nowait, b"__END__")

    async def handle_tts(self, websocket):
        logger.info(f"ÂÆ¢Êà∑Á´ØËøûÊé•: {websocket.remote_address}")
        loop = asyncio.get_running_loop()

        # Êô∫ËÉΩÊãºÂè•ÁºìÂÜ≤Âå∫
        sentence_buffer = ""

        current_job_id = None
        cancel_event = threading.Event()
        audio_queue = asyncio.Queue()

        async def _stop_current_job():
            nonlocal current_job_id, cancel_event,sentence_buffer
            cancel_event.set()
            current_job_id = None
            cancel_event = threading.Event()
            while not audio_queue.empty():
                try:
                    audio_queue.get_nowait()
                except:
                    break
            sentence_buffer = ""

        # ÂèëÈÄÅÂæ™ÁéØ
        async def _sender_loop():
            while True:
                try:
                    chunk = await audio_queue.get()
                    if chunk == b"__END__":
                        if current_job_id:
                            # ÈÄÇÈÖç N.E.K.O ÂÆ¢Êà∑Á´ØÂçèËÆÆ
                            response_done = {"type": "response.done", "job_id": current_job_id}
                            await websocket.send(json.dumps(response_done))
                        continue
                    await websocket.send(chunk)
                except Exception:
                    break

        sender_task = asyncio.create_task(_sender_loop())

        try:
            # ÂèëÈÄÅ ready ‰ø°Âè∑
            await websocket.send(json.dumps({"type": "ready"}))

            async for message in websocket:
                if isinstance(message, bytes): continue
                try:
                    data = json.loads(message)
                except:
                    continue

                msg_type = data.get("type")
                if "text" in data and not msg_type: msg_type = "legacy.text"

                if msg_type == "input_text_buffer.append":
                    text_fragment = data.get("text", "")
                    sentence_buffer += text_fragment

                elif msg_type in ("input_text_buffer.commit", "legacy.text"):
                    if msg_type == "legacy.text":
                        sentence_buffer += data.get("text", "")

                    # Ê≠£ÂàôÊô∫ËÉΩÊñ≠Âè•
                    parts = re.split(r'([„ÄÇÔºÅÔºü.!?\n]+)', sentence_buffer)

                    if len(parts) > 1:
                        for i in range(0, len(parts) - 1, 2):
                            sentence = parts[i] + parts[i + 1]
                            sentence = sentence.strip()
                            if not sentence: continue

                            if not current_job_id:
                                current_job_id = str(uuid.uuid4())
                                await websocket.send(json.dumps({"type": "response.start", "job_id": current_job_id}))

                            logger.info(f"üì• Âè•Â≠ê: {sentence[:20]}...")
                            self.task_queue.put((sentence, current_job_id, loop, audio_queue, cancel_event))

                        sentence_buffer = parts[-1]

                    # ÁºìÂÜ≤Âå∫ÂÖúÂ∫ï (Èò≤Ê≠¢‰∏ÄÁõ¥‰∏çËØ¥ËØù)
                    if len(sentence_buffer) > self.buffer_fallback_chars:
                        sentence = sentence_buffer
                        sentence_buffer = ""
                        if not current_job_id:
                            current_job_id = str(uuid.uuid4())
                            await websocket.send(json.dumps({"type": "response.start", "job_id": current_job_id}))
                        self.task_queue.put((sentence, current_job_id, loop, audio_queue, cancel_event))

                elif msg_type == "cancel":
                    await _stop_current_job()

        finally:
            await _stop_current_job()
            sender_task.cancel()


async def main():
    tts_custom = {}
    repo_root = None
    try:
        p = Path(__file__).resolve()
        config_path = None
        for parent in [p.parent] + list(p.parents):
            candidate = parent / "config" / "api_providers.json"
            if candidate.exists():
                config_path = candidate
                break
        if config_path is not None:
            with config_path.open("r", encoding="utf-8") as f:
                cfg = json.load(f)
            tts_custom = cfg.get("tts_custom", {}) or {}
            repo_root = config_path.parent.parent
    except Exception:
        tts_custom = {}
        repo_root = None

    model_path = tts_custom.get("model_path") or "/home/amadeus/models/qwen3_tts"
    host = tts_custom.get("host") or "0.0.0.0"
    port = int(tts_custom.get("port") or 8765)

    voice_pt_path = tts_custom.get("voice_pt_path")
    if voice_pt_path:
        p = Path(voice_pt_path)
        if not p.is_absolute() and repo_root is not None:
            p = (repo_root / p).resolve()
        voice_pt_path = str(p)

    ref_wav = tts_custom.get("ref_wav")
    if ref_wav:
        p = Path(ref_wav)
        if not p.is_absolute() and repo_root is not None:
            p = (repo_root / p).resolve()
        ref_wav = str(p)

    ref_text = tts_custom.get("ref_text")
    language = tts_custom.get("language")
    chunk_size = tts_custom.get("chunk_size")
    buffer_fallback_chars = tts_custom.get("buffer_fallback_chars")

    server = QwenLocalServer(
        model_path,
        voice_pt_path=voice_pt_path,
        ref_wav=ref_wav,
        ref_text=ref_text,
        language=language,
        chunk_size=chunk_size,
        buffer_fallback_chars=buffer_fallback_chars,
    )

    async with websockets.serve(server.handle_tts, host, port):
        logger.info(f"üöÄ Êú¨Âú∞ TTS ÊúçÂä°Â∑≤ÂêØÂä®: ws://{host}:{port}")
        await asyncio.Future()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass
