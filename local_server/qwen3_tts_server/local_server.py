import asyncio
import websockets
import json
import logging
import torch
import sys
import os
import time
import threading
import uuid
import queue
import numpy as np
import re

# ========================================================
# 1. åˆå§‹åŒ– Logging
# ========================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [TTS-Server] %(levelname)s - %(message)s'
)
logger = logging.getLogger("Qwen3-TTS-Server")

# ========================================================
# 2. è·¯å¾„é…ç½® (é€‚é… Linux åŸç”Ÿè·¯å¾„)
# ========================================================
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
# å‡è®¾ä½ è¿˜åœ¨ WSL çš„è¿™ä¸ªä½ç½®
MODEL_SOURCE_DIR = os.path.join(PROJECT_ROOT, "Qwen3-TTS")

if MODEL_SOURCE_DIR not in sys.path:
    sys.path.append(MODEL_SOURCE_DIR)

try:
    from qwen_tts.core.models.modeling_qwen3_tts import Qwen3TTSForConditionalGeneration
    from qwen_tts.core.models.processing_qwen3_tts import Qwen3TTSProcessor
    from qwen_tts.inference.qwen3_tts_model import Qwen3TTSModel, VoiceClonePromptItem

    torch.serialization.add_safe_globals([VoiceClonePromptItem])
    logger.info("âœ… æˆåŠŸå¯¼å…¥ Qwen3-TTS åŸç”Ÿç»„ä»¶")
except ImportError as e:
    logger.error(f"âŒ ç»„ä»¶å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)


# ========================================================
# 3. Server ç±»å®šä¹‰ (èåˆç‰ˆ)
# ========================================================
class QwenLocalServer:
    def __init__(self, model_path):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.pt_path = os.path.join(PROJECT_ROOT, "nyaning_voice.pt")
        self.model = None
        self.cached_prompt = None
        self.pad_token_id = None
        self.ref_text = "ã‚¢ãƒ©ãƒãƒ ã‚·ãƒ¥ãƒ¼ ãƒ ã‚µã‚¤ãƒ€ã‚¤ ãƒˆã‚· ãƒ¯ ãƒãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ  ãƒ‡ ã‚¢ãƒ«ã€‚"

        # --- ä»»åŠ¡é˜Ÿåˆ— (è§£å†³å¹¶å‘å¡é¡¿) ---
        self.task_queue = queue.Queue()
        # å¯åŠ¨åå°å·¥äºº
        threading.Thread(target=self._worker_loop, daemon=True).start()

        self._load_engine(model_path)

    def _load_engine(self, model_path):
        try:
            t0 = time.time()
            logger.info(f"æ­£åœ¨å¯åŠ¨ N.E.K.O è¯­éŸ³å¼•æ“ (Device: {self.device})...")

            processor = Qwen3TTSProcessor.from_pretrained(model_path, fix_mistral_regex=True)

            # --- å…³é”®ä¼˜åŒ– 1: æ˜¾å¼æŒ‡å®š Bfloat16 å’Œ Flash Attention 2 ---
            raw_model = Qwen3TTSForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,  # ç»™ transformers çœ‹
                dtype=torch.bfloat16,  # ç»™ qwen çœ‹
                attn_implementation="flash_attention_2",
                device_map="cuda",
                low_cpu_mem_usage=True
            )
            raw_model.eval()

            self.model = Qwen3TTSModel(model=raw_model, processor=processor)

            # --- å…³é”®ä¼˜åŒ– 2: é¢„è®¾ Config é˜²æ­¢æ¨ç†æ—¶åå¤åˆå§‹åŒ– ---
            self.pad_token_id = raw_model.config.pad_token_id or raw_model.config.eos_token_id
            if hasattr(self.model, 'generation_config'):
                self.model.generation_config.pad_token_id = self.pad_token_id

            # åŠ è½½/æå–éŸ³è‰²
            if os.path.exists(self.pt_path):
                logger.info(f"âœ¨ å‘ç°éŸ³è‰²ç‰¹å¾ {self.pt_path}ï¼ŒåŠ è½½ä¸­...")
                self.cached_prompt = torch.load(self.pt_path, map_location=self.device, weights_only=False)
            else:
                logger.warning("ğŸ™ï¸ æœªå‘ç°éŸ³è‰²ç‰¹å¾ï¼Œå°è¯•æå–...")
                ref_wav = os.path.join(PROJECT_ROOT, "uttid_f1.wav")
                if os.path.exists(ref_wav):
                    with torch.no_grad():
                        with torch.amp.autocast('cuda', dtype=torch.bfloat16):
                            self.cached_prompt = self.model.create_voice_clone_prompt(
                                ref_audio=ref_wav, ref_text=self.ref_text
                            )
                    torch.save(self.cached_prompt, self.pt_path)
                    logger.info("âœ… éŸ³è‰²æå–å®Œæˆ")
                else:
                    logger.error(f"âŒ æ‰¾ä¸åˆ°å‚è€ƒéŸ³é¢‘: {ref_wav}")

            logger.info(f"ğŸš€ å¼•æ“å°±ç»ª | ç²¾åº¦: {raw_model.dtype} | è€—æ—¶: {time.time() - t0:.2f}s")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å¼•æ“å¼‚å¸¸: {e}")

    def _worker_loop(self):
        logger.info("ğŸ‘· æ™ºèƒ½æ‹¼å¥é˜Ÿåˆ—æœåŠ¡å·²å¯åŠ¨")
        while True:
            task = self.task_queue.get()
            if task is None: break

            full_text, job_id, loop, audio_queue, cancel_event = task

            if cancel_event.is_set():
                self.task_queue.task_done()
                continue

            self._do_inference(full_text, job_id, loop, audio_queue, cancel_event)
            self.task_queue.task_done()

    def _do_inference(self, full_text, job_id, loop, audio_queue, cancel_event):
        try:
            if not self.model or self.cached_prompt is None: return

            start_time = time.time()

            # --- å…³é”®ä¼˜åŒ– 3: ä½¿ç”¨ inference_mode å’Œ autocast ---
            with torch.inference_mode():
                with torch.amp.autocast('cuda', dtype=torch.bfloat16):
                    wavs, sr = self.model.generate_voice_clone(
                        text=full_text,
                        voice_clone_prompt=self.cached_prompt,
                        language="Chinese",
                        pad_token_id=self.pad_token_id
                    )

            if torch.cuda.is_available():
                torch.cuda.synchronize()

            inference_duration = time.time() - start_time

            # --- ğŸš¨ æ ¸å¿ƒä¿®å¤ï¼šæ‰¾å›ä¸¢å¤±çš„ Int16 è½¬æ¢ (è¿™å°±æ˜¯æ²¡å£°éŸ³çš„åŸå› ï¼) ---
            # å…”è€å¸ˆçš„ä»£ç å¯èƒ½ç›´æ¥å‘äº† floatï¼Œæˆ‘ä»¬éœ€è¦è½¬å› int16
            audio_data = wavs[0].flatten()
            audio_int16 = (audio_data * 32767).astype(np.int16)

            audio_real_duration = len(audio_int16) / sr
            rtf = inference_duration / audio_real_duration if audio_real_duration > 0 else 0

            logger.info(
                f"âœ… [{job_id}] å®Œæˆ | è€—æ—¶:{inference_duration:.3f}s | éŸ³é¢‘:{audio_real_duration:.2f}s | RTF:{rtf:.4f}"
            )

            # å‘é€æ•°æ®
            chunk_size = 4096
            for i in range(0, len(audio_int16), chunk_size):
                if cancel_event.is_set(): break
                chunk = audio_int16[i:i + chunk_size].tobytes()
                loop.call_soon_threadsafe(audio_queue.put_nowait, chunk)

        except Exception as e:
            logger.error(f"âŒ æ¨ç†é”™è¯¯: {e}")
            import traceback
            logger.error(traceback.format_exc())
        finally:
            loop.call_soon_threadsafe(audio_queue.put_nowait, b"__END__")

    async def handle_tts(self, websocket):
        logger.info(f"å®¢æˆ·ç«¯è¿æ¥: {websocket.remote_address}")
        loop = asyncio.get_running_loop()

        # æ™ºèƒ½æ‹¼å¥ç¼“å†²åŒº
        self.sentence_buffer = ""

        current_job_id = None
        cancel_event = threading.Event()
        audio_queue = asyncio.Queue()

        async def _stop_current_job():
            nonlocal current_job_id, cancel_event
            cancel_event.set()
            current_job_id = None
            cancel_event = threading.Event()
            while not audio_queue.empty():
                try:
                    audio_queue.get_nowait()
                except:
                    break
            self.sentence_buffer = ""

        # å‘é€å¾ªç¯
        async def _sender_loop():
            while True:
                try:
                    chunk = await audio_queue.get()
                    if chunk == b"__END__":
                        if current_job_id:
                            # é€‚é… N.E.K.O å®¢æˆ·ç«¯åè®®
                            response_done = {"type": "response.done", "job_id": current_job_id}
                            await websocket.send(json.dumps(response_done))
                        continue
                    await websocket.send(chunk)
                except Exception:
                    break

        sender_task = asyncio.create_task(_sender_loop())

        try:
            # å‘é€ ready ä¿¡å·
            await websocket.send(json.dumps({"type": "ready"}))

            async for message in websocket:
                if isinstance(message, bytes): continue
                try:
                    data = json.loads(message)
                except:
                    continue

                msg_type = data.get("type")
                if "text" in data and not msg_type: msg_type = "legacy.text"

                if msg_type == "input_text_buffer.append":
                    text_fragment = data.get("text", "")
                    self.sentence_buffer += text_fragment

                elif msg_type in ("input_text_buffer.commit", "legacy.text"):
                    if msg_type == "legacy.text":
                        self.sentence_buffer += data.get("text", "")

                    # æ­£åˆ™æ™ºèƒ½æ–­å¥
                    parts = re.split(r'([ã€‚ï¼ï¼Ÿ.!?\n]+)', self.sentence_buffer)

                    if len(parts) > 1:
                        for i in range(0, len(parts) - 1, 2):
                            sentence = parts[i] + parts[i + 1]
                            sentence = sentence.strip()
                            if not sentence: continue

                            if not current_job_id:
                                current_job_id = str(uuid.uuid4())
                                await websocket.send(json.dumps({"type": "response.start", "job_id": current_job_id}))

                            logger.info(f"ğŸ“¥ å¥å­: {sentence[:20]}...")
                            self.task_queue.put((sentence, current_job_id, loop, audio_queue, cancel_event))

                        self.sentence_buffer = parts[-1]

                    # ç¼“å†²åŒºå…œåº• (é˜²æ­¢ä¸€ç›´ä¸è¯´è¯)
                    if len(self.sentence_buffer) > 30:
                        sentence = self.sentence_buffer
                        self.sentence_buffer = ""
                        if not current_job_id:
                            current_job_id = str(uuid.uuid4())
                            await websocket.send(json.dumps({"type": "response.start", "job_id": current_job_id}))
                        self.task_queue.put((sentence, current_job_id, loop, audio_queue, cancel_event))

                elif msg_type == "cancel":
                    await _stop_current_job()

        finally:
            await _stop_current_job()
            sender_task.cancel()


async def main():
    # è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®
    MODEL_PATH = "/home/amadeus/models/qwen3_tts"
    server = QwenLocalServer(MODEL_PATH)
    async with websockets.serve(server.handle_tts, "0.0.0.0", 8765):
        logger.info("ğŸš€ æœ¬åœ° TTS æœåŠ¡å·²å¯åŠ¨: ws://localhost:8765")
        await asyncio.Future()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass
