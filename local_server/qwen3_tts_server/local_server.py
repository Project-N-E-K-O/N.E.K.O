import asyncio
import websockets
import json
import logging
import torch
import sys
import os
import time
import threading
import uuid
import numpy as np

# ========================================================
# 1. åˆå§‹åŒ– Logging (è§£å†³ NameError)
# ========================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [TTS-Server] %(levelname)s - %(message)s'
)
logger = logging.getLogger("Qwen3-TTS-Server")

# ========================================================
# 2. è·¯å¾„ä¸ç¯å¢ƒé…ç½®
# ========================================================
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
MODEL_SOURCE_DIR = os.path.join(PROJECT_ROOT, "Qwen3-TTS")

if MODEL_SOURCE_DIR not in sys.path:
    sys.path.append(MODEL_SOURCE_DIR)

try:
    # ä½¿ç”¨ demo ä¸­éªŒè¯æˆåŠŸçš„å¯¼å…¥è·¯å¾„
    from qwen_tts.core.models.modeling_qwen3_tts import Qwen3TTSForConditionalGeneration
    from qwen_tts.core.models.processing_qwen3_tts import Qwen3TTSProcessor
    from qwen_tts.inference.qwen3_tts_model import Qwen3TTSModel
    # å…ˆä»æ¨¡å‹åº“å¯¼å…¥è¿™ä¸ªç±»
    from qwen_tts.inference.qwen3_tts_model import VoiceClonePromptItem
    torch.serialization.add_safe_globals([VoiceClonePromptItem])
    logger.info("âœ… æˆåŠŸå¯¼å…¥ Qwen3-TTS åŸç”Ÿç»„ä»¶")
except ImportError as e:
    logger.error(f"âŒ ç»„ä»¶å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)


# ========================================================
# 3. QwenLocalServer ç±»å®šä¹‰
# ========================================================
class QwenLocalServer:
    def __init__(self, model_path):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.pt_path = os.path.join(PROJECT_ROOT, "nyaning_voice.pt")
        self.model = None
        self.cached_prompt = None

        # å¯¹åº”ä½ éŸ³é¢‘ uttid_f1.wav çš„æ—¥è¯­åŸæ–‡
        self.ref_text = "ã‚¢ãƒ©ãƒãƒ ã‚·ãƒ¥ãƒ¼ ãƒ ã‚µã‚¤ãƒ€ã‚¤ ãƒˆã‚· ãƒ¯ ãƒãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ  ãƒ‡ ã‚¢ãƒ«ã€‚"

        self._load_engine(model_path)

    def _load_engine(self, model_path):
        try:
            t0 = time.time()
            logger.info(f"æ­£åœ¨å¯åŠ¨ N.E.K.O è¯­éŸ³å¼•æ“ (Device: {self.device})...")

            # A. åŠ è½½å¤„ç†å™¨ (ä¿®å¤æ­£åˆ™è­¦å‘Š)
            processor = Qwen3TTSProcessor.from_pretrained(model_path, fix_mistral_regex=True)

            # B. åŠ è½½æ¨¡å‹ (å¼€å¯ FlashAttention 2.0)
            dtype = torch.bfloat16 if self.device == "cuda" else torch.float32
            raw_model = Qwen3TTSForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=dtype,
                attn_implementation="flash_attention_2" if self.device == "cuda" else "eager",
                low_cpu_mem_usage=True
            ).to(self.device)

            # C. å°è£…æ¨ç†åŒ…è£…å™¨
            self.model = Qwen3TTSModel(model=raw_model, processor=processor)
            self.pad_token_id = raw_model.config.pad_token_id or raw_model.config.eos_token_id

            # D. é¢„åŠ è½½/å¯¼å‡ºéŸ³è‰²ç‰¹å¾ (ç§’å¼€ä¼˜åŒ–)
            if os.path.exists(self.pt_path):
                logger.info(f"âœ¨ å‘ç°éŸ³è‰²ç‰¹å¾ {self.pt_path}ï¼Œæ‰§è¡Œç§’é€ŸåŠ è½½")
                self.cached_prompt = torch.load(self.pt_path, map_location=self.device, weights_only=False)
            else:
                logger.warning("ğŸ™ï¸ æœªå‘ç°å¯¼å‡ºçš„éŸ³è‰²ï¼Œå°†å°è¯•ä» uttid_f1.wav æå–...")
                ref_wav = os.path.join(PROJECT_ROOT, "uttid_f1.wav")
                if os.path.exists(ref_wav):
                    with torch.no_grad():
                        self.cached_prompt = self.model.create_voice_clone_prompt(
                            ref_audio=ref_wav,
                            ref_text=self.ref_text
                        )
                    torch.save(self.cached_prompt, self.pt_path)
                    logger.info("âœ… éŸ³è‰²æå–å¹¶ä¿å­˜å®Œæˆ")
                else:
                    logger.error(f"âŒ æ‰¾ä¸åˆ°å‚è€ƒéŸ³é¢‘: {ref_wav}")

            logger.info(f"ğŸš€ è¯­éŸ³å¼•æ“åˆå§‹åŒ–å®Œæˆï¼Œæ€»è€—æ—¶: {time.time() - t0:.2f}s")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å¼•æ“å¼‚å¸¸: {e}")
            import traceback
            logger.error(traceback.format_exc())

    async def handle_tts(self, websocket):
        logger.info(f"å®¢æˆ·ç«¯å·²è¿æ¥: {websocket.remote_address}")
        loop = asyncio.get_running_loop()

        text_buffer = ""
        session_cfg = {"voice": None, "sample_rate": 24000}
        current_job_id = None
        cancel_event = threading.Event()
        audio_queue = asyncio.Queue()

        async def _stop_current_job():
            nonlocal current_job_id, cancel_event
            cancel_event.set()
            current_job_id = None
            cancel_event = threading.Event()
            while not audio_queue.empty():
                try:
                    audio_queue.get_nowait()
                except Exception:
                    break

        def _producer_wrapper(full_text, job_id):
            try:
                if not self.model or self.cached_prompt is None:
                    logger.error("æ¨¡å‹æˆ–éŸ³è‰²æœªå°±ç»ªï¼Œæ— æ³•åˆæˆ")
                    return

                start_time = time.time()
                logger.info(f"ğŸ¤ [{job_id}] æ­£åœ¨åˆæˆæ–‡æœ¬: {full_text[:30]}...")

                with torch.no_grad():
                    # è°ƒç”¨ generate_voice_clone
                    # æ³¨æ„ï¼šå¦‚æœå¼€å‘è€…æ²¡æä¾› generate_streamï¼Œè¿™é‡Œç”Ÿæˆå®Œæ•´éŸ³é¢‘åå†åˆ†å—
                    wavs, sr = self.model.generate_voice_clone(
                        text=full_text,
                        voice_clone_prompt=self.cached_prompt,
                        language="Chinese"
                    )

                    # å¼ºåˆ¶ GPU åŒæ­¥ä»¥è·å–ç²¾ç¡®çš„æ¨ç†è®¡æ—¶
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()

                    # --- 2. è®¡ç®—è€—æ—¶ä¸æ€§èƒ½æŒ‡æ ‡ ---
                    end_time = time.time()
                    inference_duration = end_time - start_time

                    # ã€å…³é”®ä¿®æ­£ç‚¹ã€‘ï¼šwavs[0] å·²ç»æ˜¯ numpy æ•°ç»„ï¼Œç›´æ¥ä½¿ç”¨
                    audio_data = wavs[0].flatten()

                    # å½’ä¸€åŒ–å¹¶è½¬ä¸º 16-bit PCM (Int16) ä¿®å¤é›ªèŠ±éŸ³
                    audio_int16 = (audio_data * 32767).astype(np.int16)

                    # è®¡ç®—éŸ³é¢‘å®é™…æ—¶é•¿ï¼ˆç§’ï¼‰
                    audio_real_duration = len(audio_int16) / sr
                    # è®¡ç®— RTF (å®æ—¶ç‡)ï¼Œæ•°å€¼è¶Šå°æ€§èƒ½è¶Šå¼º
                    rtf = inference_duration / audio_real_duration if audio_real_duration > 0 else 0

                    # --- 3. æŠ¥å‘Šåˆæˆç»“æŸ ---
                    logger.info(
                        f"âœ… [{job_id}] åˆæˆä»»åŠ¡å®Œæˆï¼\n"
                        f"   ----------------------------------------\n"
                        f"   â±ï¸ æ¨ç†è€—æ—¶: {inference_duration:.3f} ç§’\n"
                        f"   ğŸ”Š éŸ³é¢‘é•¿åº¦: {audio_real_duration:.2f} ç§’\n"
                        f"   ğŸš€ å®æ—¶ç‡ (RTF): {rtf:.4f} {'(æé€Ÿ)' if rtf < 0.2 else ''}\n"
                        f"   ----------------------------------------"
                    )

                    # 4. åˆ†å—æ¨é€åˆ° WebSocket å‘é€é˜Ÿåˆ—
                    chunk_size = 2048
                    for i in range(0, len(audio_int16), chunk_size):
                        if cancel_event.is_set():
                            logger.warning(f"âš ï¸ [{job_id}] ä»»åŠ¡è¢«å–æ¶ˆ")
                            break
                        chunk = audio_int16[i:i+chunk_size].tobytes()
                        # å°†å­—èŠ‚æµæ”¾å…¥å¼‚æ­¥é˜Ÿåˆ—
                        loop.call_soon_threadsafe(audio_queue.put_nowait, chunk)

            except Exception as e:
                logger.error(f"æ¨ç†å‡ºé”™: {e}")
            finally:
                loop.call_soon_threadsafe(audio_queue.put_nowait, b"__END__")
                # åŠæ—¶æ¸…ç†æ˜¾å­˜ç¢ç‰‡
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

        async def _sender_loop():
            while True:
                chunk = await audio_queue.get()
                if chunk == b"__END__":
                    await websocket.send(json.dumps({"type": "response.done", "job_id": current_job_id}))
                    continue
                try:
                    await websocket.send(chunk)
                except Exception:
                    break

        sender_task = asyncio.create_task(_sender_loop())

        try:
            await websocket.send(json.dumps({"type": "ready"}))
            async for message in websocket:
                if isinstance(message, bytes): continue
                try:
                    data = json.loads(message)
                except:
                    continue

                msg_type = data.get("type")

                # å…¼å®¹æ—§æ ¼å¼æˆ– text ç›´æ¥å‘é€
                if "text" in data and not msg_type:
                    msg_type = "legacy.text"

                if msg_type == "input_text_buffer.append":
                    text_buffer += data.get("text", "")
                elif msg_type in ("input_text_buffer.commit", "legacy.text"):
                    if msg_type == "legacy.text":
                        text_buffer = data.get("text", "")

                    full_text = text_buffer.strip()
                    text_buffer = ""
                    if not full_text: continue

                    await _stop_current_job()
                    current_job_id = str(uuid.uuid4())
                    logger.info(f"æ”¶åˆ°è¯·æ±‚ job_id={current_job_id}: {full_text[:30]}...")

                    await websocket.send(json.dumps({
                        "type": "response.start",
                        "job_id": current_job_id
                    }))

                    threading.Thread(target=_producer_wrapper, args=(full_text, current_job_id), daemon=True).start()

                elif msg_type == "cancel":
                    await _stop_current_job()

        finally:
            await _stop_current_job()
            sender_task.cancel()


async def main():
    MODEL_PATH = "/home/amadeus/models/qwen3_tts"
    # MODEL_PATH = "/mnt/h/pr/N.E.K.O/local_server/qwen3_tts_server/Qwen3-TTS/pretrained_model/Qwen3-TTS-12Hz-1.7B-Base"
    server_instance = QwenLocalServer(MODEL_PATH)
    async with websockets.serve(server_instance.handle_tts, "0.0.0.0", 8765):
        logger.info("ğŸš€ æœ¬åœ° TTS æœåŠ¡å·²å¯åŠ¨: ws://localhost:8765")
        await asyncio.Future()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("æœåŠ¡å™¨åœæ­¢è¿è¡Œ")